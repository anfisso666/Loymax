# Standard libraries
from typing import Dict, List, Optional, Tuple
import warnings

# Data manipulation and analysis
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Statistical analysis
from scipy import stats
from scipy.stats import pearsonr
from statsmodels.stats.weightstats import ttest_ind
from statsmodels.regression.linear_model import OLS
from statsmodels.tools.tools import add_constant

# Machine learning
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import (
    RandomForestRegressor, 
    RandomForestClassifier
)
from sklearn.model_selection import (
    cross_val_score, 
    train_test_split
)
from sklearn.metrics import (
    r2_score, 
    mean_squared_error, 
    accuracy_score
)
from sklearn.decomposition import PCA

# Specialized libraries
import loymax as lm
from pylift.eval import UpliftEval

# Suppress warnings
warnings.filterwarnings('ignore')

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ pylift —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
try:
    from pylift.eval import UpliftEval
    from pylift import TransformedOutcome
    PYLIFT_AVAILABLE = True
except ImportError:
    PYLIFT_AVAILABLE = False
    print("‚ö†Ô∏è  PyLift –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã.")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PyLift
try:
    from pylift import TransformedOutcome
    PYLIFT_AVAILABLE = True
except ImportError:
    PYLIFT_AVAILABLE = False
    print("‚ö†Ô∏è PyLift –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±—É–¥—É—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")

try:
    import statsmodels.api as sm
    from statsmodels.regression.linear_model import OLS
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    print("‚ö†Ô∏è Statsmodels –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã.")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

# 1. –ù–ê–•–û–î–ò–ú –ü–ï–†–ò–û–î–´ –°–ö–†–´–¢–´–• –ê–ö–¶–ò–ô
def find_hidden_campaigns(purchases):
    """–ò—â–µ–º –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –¥–Ω–∏ –ø–æ –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è–º –Ω–∞ –∫–∞—Ä—Ç—ã"""
    daily_stats = purchases.groupby(purchases['Opetation_datetime'].dt.date).agg({
        'toCard_stand': 'sum',
        'fromCard_stand': 'sum',
        'Person_BKEY': 'nunique',
        'Amount_Cheque': 'sum'
    }).reset_index()
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—ã–±—Ä–æ—Å—ã (—Ç–æ–ø 5% –¥–Ω–µ–π –ø–æ –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è–º)
    threshold = daily_stats['toCard_stand'].quantile(0.95)
    campaign_days = daily_stats[daily_stats['toCard_stand'] > threshold]['Opetation_datetime']
    
    return campaign_days.tolist(), daily_stats

# 2. –ê–ù–ê–õ–ò–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô –í –ü–û–ö–£–ü–ö–ê–•
def analyze_purchase_changes(purchases, goods, campaign_days):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ –¥–æ/–≤–æ –≤—Ä–µ–º—è/–ø–æ—Å–ª–µ –∫–∞–º–ø–∞–Ω–∏–π"""
    
    results = []
    
    for campaign_day in campaign_days[:3]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –∫–∞–º–ø–∞–Ω–∏–∏
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥—ã
        before_start = campaign_day - timedelta(days=7)
        before_end = campaign_day - timedelta(days=1)
        during_start = campaign_day
        during_end = campaign_day + timedelta(days=3)
        after_start = during_end + timedelta(days=1)
        after_end = after_start + timedelta(days=7)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º
        periods = {
            'before': (before_start, before_end),
            'during': (during_start, during_end), 
            'after': (after_start, after_end)
        }
        
        for period_name, (start, end) in periods.items():
            mask = (purchases['Opetation_datetime'].dt.date >= start) & \
                   (purchases['Opetation_datetime'].dt.date <= end)
            
            period_data = purchases[mask].merge(goods, on='Goods_BKEY', how='left')
            
            # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = {
                'campaign_date': campaign_day,
                'period': period_name,
                'unique_customers': period_data['Person_BKEY'].nunique(),
                'avg_basket_size': period_data.groupby('Purchase_ID')['Qnt'].sum().mean(),
                'avg_cheque': period_data['Amount_Cheque'].mean(),
                'unique_products': period_data['Goods_BKEY'].nunique(),
                'unique_categories': period_data['cat_lev_02_BKEY'].nunique()
            }
            
            results.append(metrics)
    
    return pd.DataFrame(results)

# 3. –¢–û–ü –ò–ó–ú–ï–ù–ï–ù–ò–Ø –í –ö–ê–¢–ï–ì–û–†–ò–Ø–•
def analyze_category_shifts(purchases, goods, campaign_days):
    """–ö–∞–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –∫–∞–º–ø–∞–Ω–∏–π"""
    
    category_changes = []
    
    for campaign_day in campaign_days[:2]:
        # –î–æ –∫–∞–º–ø–∞–Ω–∏–∏
        before_mask = (purchases['Opetation_datetime'].dt.date >= campaign_day - timedelta(days=7)) & \
                     (purchases['Opetation_datetime'].dt.date < campaign_day)
        
        # –í–æ –≤—Ä–µ–º—è –∫–∞–º–ø–∞–Ω–∏–∏  
        during_mask = (purchases['Opetation_datetime'].dt.date >= campaign_day) & \
                     (purchases['Opetation_datetime'].dt.date <= campaign_day + timedelta(days=3))
        
        before_data = purchases[before_mask].merge(goods, on='Goods_BKEY')
        during_data = purchases[during_mask].merge(goods, on='Goods_BKEY')
        
        # –°—á–∏—Ç–∞–µ–º –¥–æ–ª—é –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        before_cat = before_data.groupby('cat_lev_02_BKEY')['Amount'].sum()
        during_cat = during_data.groupby('cat_lev_02_BKEY')['Amount'].sum()
        
        before_pct = before_cat / before_cat.sum() * 100
        during_pct = during_cat / during_cat.sum() * 100
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª—å—à–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        change = during_pct - before_pct
        top_changes = change.abs().nlargest(5)
        
        for cat, change_val in top_changes.items():
            category_changes.append({
                'campaign_date': campaign_day,
                'category': cat,
                'change_percent': change_val,
                'before_share': before_pct.get(cat, 0),
                'during_share': during_pct.get(cat, 0)
            })
    
    return pd.DataFrame(category_changes)

# 4. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
def create_visualizations(daily_stats, analysis_results, category_changes):
    """–°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –î–∏–Ω–∞–º–∏–∫–∞ –Ω–∞—á–∏—Å–ª–µ–Ω–∏–π (–ø–æ–∏—Å–∫ –∫–∞–º–ø–∞–Ω–∏–π)
    axes[0,0].plot(daily_stats['Opetation_datetime'], daily_stats['toCard_stand'])
    axes[0,0].set_title('–ù–∞—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞ –∫–∞—Ä—Ç—ã –ø–æ –¥–Ω—è–º (–ø–∏–∫–∏ = —Å–∫—Ä—ã—Ç—ã–µ –∞–∫—Ü–∏–∏)')
    axes[0,0].tick_params(axis='x', rotation=45)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º
    period_order = ['before', 'during', 'after']
    avg_by_period = analysis_results.groupby('period')['avg_cheque'].mean().reindex(period_order)
    axes[0,1].bar(avg_by_period.index, avg_by_period.values)
    axes[0,1].set_title('–°—Ä–µ–¥–Ω–∏–π —á–µ–∫: –¥–æ/–≤–æ –≤—Ä–µ–º—è/–ø–æ—Å–ª–µ –∫–∞–º–ø–∞–Ω–∏–π')
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞–∑–º–µ—Ä –∫–æ—Ä–∑–∏–Ω—ã
    basket_by_period = analysis_results.groupby('period')['avg_basket_size'].mean().reindex(period_order)
    axes[1,0].bar(basket_by_period.index, basket_by_period.values)
    axes[1,0].set_title('–†–∞–∑–º–µ—Ä –∫–æ—Ä–∑–∏–Ω—ã: –¥–æ/–≤–æ –≤—Ä–µ–º—è/–ø–æ—Å–ª–µ')
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –¢–æ–ø –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    if not category_changes.empty:
        top_cats = category_changes.nlargest(5, 'change_percent')
        axes[1,1].barh(range(len(top_cats)), top_cats['change_percent'])
        axes[1,1].set_yticks(range(len(top_cats)))
        axes[1,1].set_yticklabels([f'Cat_{int(x)}' for x in top_cats['category']])
        axes[1,1].set_title('–¢–æ–ø –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–∞—Ç–µ–≥–æ—Ä–∏–π (%)')
    
    plt.tight_layout()
    plt.show()

class ATENIVAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è (ATE) 
    —Å –ø–æ–º–æ—â—å—é NIV (Net Information Value) –Ω–∞ –¥–∞–Ω–Ω—ã—Ö A/B-—Ç–µ—Å—Ç–æ–≤
    """
    
    def __init__(self, data: pd.DataFrame, 
                 treatment_col: str = 'treatment',
                 outcome_col: str = 'conversion',
                 feature_cols: List[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            data: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            treatment_col: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è
            outcome_col: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            feature_cols: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        self.data = data.copy()
        self.treatment_col = treatment_col
        self.outcome_col = outcome_col
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if feature_cols is None:
            self.feature_cols = [f'f{i}' for i in range(12)]  # f0-f11
        else:
            self.feature_cols = feature_cols
            
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è IV –∞–Ω–∞–ª–∏–∑–∞
        self.network_vars = ['visit', 'exposure'] if 'visit' in data.columns and 'exposure' in data.columns else []
        
        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞:")
        print(f"   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {data.shape}")
        print(f"   –°—Ç–æ–ª–±–µ—Ü –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è: {treatment_col}")
        print(f"   –°—Ç–æ–ª–±–µ—Ü –∏—Å—Ö–æ–¥–∞: {outcome_col}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_cols)}")
        print(f"   –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {self.network_vars}")
    
    def calculate_niv(self, feature_col: str, n_bins: int = 10) -> Dict:
        """
        –†–∞—Å—á–µ—Ç Net Information Value (NIV) –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞
        
        Args:
            feature_col: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –ø—Ä–∏–∑–Ω–∞–∫–æ–º
            n_bins: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ NIV –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –±–∏–Ω—ã –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            feature_values = self.data[feature_col].copy()
            
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∏–Ω—ã
            if len(feature_values.unique()) > n_bins:
                feature_binned = pd.cut(feature_values, bins=n_bins, duplicates='drop')
            else:
                feature_binned = feature_values
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∫–æ–Ω—Ç–∏–Ω–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            contingency_table = pd.crosstab(
                feature_binned, 
                [self.data[self.treatment_col], self.data[self.outcome_col]]
            )
            
            # –í—ã—á–∏—Å–ª—è–µ–º NIV –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
            niv_results = {}
            total_treated = self.data[self.data[self.treatment_col] == 1].shape[0]
            total_control = self.data[self.data[self.treatment_col] == 0].shape[0]
            
            for bin_value in contingency_table.index:
                # –ü–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –±–∏–Ω–∞
                treated_pos = contingency_table.loc[bin_value, (1, 1)] if (1, 1) in contingency_table.columns else 0
                treated_neg = contingency_table.loc[bin_value, (1, 0)] if (1, 0) in contingency_table.columns else 0
                control_pos = contingency_table.loc[bin_value, (0, 1)] if (0, 1) in contingency_table.columns else 0
                control_neg = contingency_table.loc[bin_value, (0, 0)] if (0, 0) in contingency_table.columns else 0
                
                # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                if treated_pos + treated_neg == 0 or control_pos + control_neg == 0:
                    continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º rates
                treated_rate = treated_pos / (treated_pos + treated_neg) if (treated_pos + treated_neg) > 0 else 0
                control_rate = control_pos / (control_pos + control_neg) if (control_pos + control_neg) > 0 else 0
                
                # –í–µ—Å–∞ –≥—Ä—É–ø–ø
                treated_weight = (treated_pos + treated_neg) / total_treated if total_treated > 0 else 0
                control_weight = (control_pos + control_neg) / total_control if total_control > 0 else 0
                
                # NIV –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –±–∏–Ω–∞
                if control_rate > 0 and treated_rate > 0:
                    niv_bin = (treated_rate - control_rate) * np.log(treated_rate / control_rate)
                else:
                    niv_bin = 0
                
                niv_results[str(bin_value)] = {
                    'treated_rate': treated_rate,
                    'control_rate': control_rate,
                    'treated_weight': treated_weight,
                    'control_weight': control_weight,
                    'niv': niv_bin,
                    'lift': treated_rate - control_rate
                }
            
            # –°—É–º–º–∞—Ä–Ω—ã–π NIV
            total_niv = sum([result['niv'] * result['treated_weight'] for result in niv_results.values()])
            
            return {
                'feature': feature_col,
                'total_niv': total_niv,
                'bin_results': niv_results,
                'n_bins': len(niv_results)
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ NIV –¥–ª—è {feature_col}: {e}")
            return {'feature': feature_col, 'total_niv': 0, 'bin_results': {}, 'n_bins': 0}
    
    def calculate_ate_simple(self) -> Dict:
        """
        –ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç ATE (—Ä–∞–∑–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–Ω–∏—Ö)
        """
        treated_outcome = self.data[self.data[self.treatment_col] == 1][self.outcome_col].mean()
        control_outcome = self.data[self.data[self.treatment_col] == 0][self.outcome_col].mean()
        
        ate = treated_outcome - control_outcome
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—à–∏–±–∫–∞
        treated_var = self.data[self.data[self.treatment_col] == 1][self.outcome_col].var()
        control_var = self.data[self.data[self.treatment_col] == 0][self.outcome_col].var()
        n_treated = self.data[self.data[self.treatment_col] == 1].shape[0]
        n_control = self.data[self.data[self.treatment_col] == 0].shape[0]
        
        se = np.sqrt(treated_var / n_treated + control_var / n_control)
        
        # t-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        t_stat = ate / se if se > 0 else 0
        p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n_treated + n_control - 2))
        
        return {
            'ate': ate,
            'se': se,
            't_stat': t_stat,
            'p_value': p_value,
            'ci_lower': ate - 1.96 * se,
            'ci_upper': ate + 1.96 * se,
            'treated_mean': treated_outcome,
            'control_mean': control_outcome
        }
    
    def two_stage_least_squares(self, instrument: str, sample_size: int = None) -> Dict:
        """
        –î–≤—É—Ö—à–∞–≥–æ–≤—ã–π –ú–ù–ö —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        """
        if not STATSMODELS_AVAILABLE:
            return self._two_stage_sklearn(instrument, sample_size)
        
        try:
            data_sample = self.data.sample(n=sample_size) if sample_size else self.data
            
            # –ü–µ—Ä–≤–∞—è —Å—Ç–∞–¥–∏—è: treatment ~ instrument + controls
            X_first_stage = sm.add_constant(data_sample[self.feature_cols + [instrument]])
            first_stage = OLS(data_sample[self.treatment_col], X_first_stage).fit()
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è treatment
            treatment_hat = first_stage.fittedvalues
            
            # –í—Ç–æ—Ä–∞—è —Å—Ç–∞–¥–∏—è: outcome ~ treatment_hat + controls
            X_second_stage = sm.add_constant(pd.concat([treatment_hat, data_sample[self.feature_cols]], axis=1))
            second_stage = OLS(data_sample[self.outcome_col], X_second_stage).fit()
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
            param_names = second_stage.params.index
            treatment_param_name = param_names[1]  # –ü–µ—Ä–≤—ã–π –ø–æ—Å–ª–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
            
            ate_estimate = second_stage.params[treatment_param_name]
            ate_se = second_stage.bse[treatment_param_name]
            
            # F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ª–∞–±–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            f_stat = first_stage.fvalue
            
            return {
                'ate': ate_estimate,
                'se': ate_se,
                't_stat': ate_estimate / ate_se if ate_se > 0 else 0,
                'p_value': 2 * (1 - stats.t.cdf(abs(ate_estimate / ate_se), len(data_sample) - len(self.feature_cols) - 2)) if ate_se > 0 else 1,
                'first_stage_f': f_stat,
                'weak_instrument': f_stat < 10,
                'instrument': instrument
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ 2SLS –¥–ª—è {instrument}: {e}")
            return self._two_stage_sklearn(instrument, sample_size)
    
    def _two_stage_sklearn(self, instrument: str, sample_size: int = None) -> Dict:
        """
        –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è 2SLS —á–µ—Ä–µ–∑ sklearn
        """
        try:
            data_sample = self.data.sample(n=sample_size) if sample_size else self.data
            
            # –ü–µ—Ä–≤–∞—è —Å—Ç–∞–¥–∏—è
            X_first = data_sample[self.feature_cols + [instrument]]
            y_first = data_sample[self.treatment_col]
            
            first_stage = LinearRegression()
            first_stage.fit(X_first, y_first)
            treatment_hat = first_stage.predict(X_first)
            
            # –í—Ç–æ—Ä–∞—è —Å—Ç–∞–¥–∏—è
            X_second = np.column_stack([treatment_hat, data_sample[self.feature_cols]])
            y_second = data_sample[self.outcome_col]
            
            second_stage = LinearRegression()
            second_stage.fit(X_second, y_second)
            
            ate_estimate = second_stage.coef_[0]  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–∏ treatment_hat
            
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –æ—à–∏–±–∫–∏
            residuals = y_second - second_stage.predict(X_second)
            mse = np.mean(residuals**2)
            ate_se = np.sqrt(mse / len(data_sample))
            
            return {
                'ate': ate_estimate,
                'se': ate_se,
                't_stat': ate_estimate / ate_se if ate_se > 0 else 0,
                'p_value': 2 * (1 - stats.t.cdf(abs(ate_estimate / ate_se), len(data_sample) - len(self.feature_cols) - 2)) if ate_se > 0 else 1,
                'first_stage_f': None,
                'weak_instrument': False,
                'instrument': instrument
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ sklearn 2SLS –¥–ª—è {instrument}: {e}")
            return {'ate': 0, 'se': 1, 't_stat': 0, 'p_value': 1, 'instrument': instrument}
    
    def pylift_analysis(self, sample_size: int = None) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é PyLift
        """
        if not PYLIFT_AVAILABLE:
            print("‚ö†Ô∏è PyLift –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return {}
        
        try:
            data_sample = self.data.sample(n=sample_size) if sample_size else self.data
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PyLift
            X = data_sample[self.feature_cols]
            y = data_sample[self.outcome_col]
            treatment = data_sample[self.treatment_col]
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å TransformedOutcome
            to_model = TransformedOutcome(X, y, treatment)
            to_model.fit()
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions = to_model.predict(X)
            
            # –í—ã—á–∏—Å–ª—è–µ–º ATE
            ate = predictions.mean()
            
            return {
                'ate': ate,
                'method': 'PyLift TransformedOutcome',
                'predictions': predictions,
                'model_score': to_model.score(X, y, treatment) if hasattr(to_model, 'score') else None
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ PyLift –∞–Ω–∞–ª–∏–∑–µ: {e}")
            return {}
    # ..
    def comprehensive_niv_ate_analysis(self, sample_size: int = None) -> Dict:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ATE —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NIV
        """
        print("üî¨ –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π NIV-ATE –∞–Ω–∞–ª–∏–∑...")
        
        results = {}
        
        # 1. –ü—Ä–æ—Å—Ç–æ–π ATE
        print("\n1Ô∏è. –ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç ATE...")
        simple_ate = self.calculate_ate_simple()
        results['simple_ate'] = simple_ate
        print(f"   ATE: {simple_ate['ate']:.6f} (SE: {simple_ate['se']:.6f}, p-value: {simple_ate['p_value']:.4f})")
        
        # 2. NIV –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("\n2Ô∏è. –†–∞—Å—á–µ—Ç NIV –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        niv_results = {}
        for feature in self.feature_cols:
    #         if feature in self.data.columns:
                niv_result = self.calculate_niv(feature)
                niv_results[feature] = niv_result
                print(f"   {feature}: NIV = {niv_result['total_niv']:.6f}")
        
        results['niv_analysis'] = niv_results
        
        # 3. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ NIV
        niv_ranking = sorted(niv_results.items(), key=lambda x: abs(x[1]['total_niv']), reverse=True)
        results['niv_ranking'] = niv_ranking
        
        print("\n–¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ NIV:")
        for i, (feature, niv_data) in enumerate(niv_ranking[:5]):
            print(f"   {i+1}. {feature}: {niv_data['total_niv']:.6f}")
        
        # 4. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        if self.network_vars:
            print("\n3Ô∏è. –ê–Ω–∞–ª–∏–∑ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏...")
            iv_results = {}
            for instrument in self.network_vars:
                if instrument in self.data.columns:
                    iv_result = self.two_stage_least_squares(instrument, sample_size)
                    iv_results[instrument] = iv_result
                    print(f"   {instrument}: ATE = {iv_result['ate']:.6f} (SE: {iv_result['se']:.6f})")
            
            results['iv_analysis'] = iv_results
        
        # 5. PyLift –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if PYLIFT_AVAILABLE:
            print("\n4Ô∏è. PyLift –∞–Ω–∞–ª–∏–∑...")
            pylift_result = self.pylift_analysis(sample_size)
            if pylift_result:
                results['pylift_analysis'] = pylift_result
                print(f"   PyLift ATE: {pylift_result['ate']:.6f}")
        
        # 6. –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüìã –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print(f"   –ü—Ä–æ—Å—Ç–æ–π ATE: {simple_ate['ate']:.6f}")
        
        if 'iv_analysis' in results:
            for instrument, iv_result in results['iv_analysis'].items():
                print(f"   2SLS ({instrument}): {iv_result['ate']:.6f}")
        
        if 'pylift_analysis' in results and results['pylift_analysis']:
            print(f"   PyLift ATE: {results['pylift_analysis']['ate']:.6f}")
        
        return results

class PyLiftNetworkAnalyzer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Network-based Weighted Outcome Estimation (NWOE) 
    —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyLift
    """
    
    def __init__(self, data: pd.DataFrame, treatment_col: str = 'treatment', 
                 outcome_col: str = 'conversion', network_vars: List[str] = ['visit', 'exposure']):
        self.data = data.copy()
        self.treatment_col = treatment_col
        self.outcome_col = outcome_col
        self.network_vars = network_vars
        self.feature_cols = [col for col in data.columns 
                           if col.startswith('f') and col not in [treatment_col, outcome_col] + network_vars]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_cols = [treatment_col, outcome_col] + network_vars
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
        
        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyLiftNetworkAnalyzer:")
        print(f"   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {self.data.shape}")
        print(f"   –§–∏—á–∏: {len(self.feature_cols)}")
        print(f"   –°–µ—Ç–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {self.network_vars}")
    
    def calculate_network_weights(self, method: str = 'exposure_based') -> np.ndarray:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ—Ç–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        
        if method == 'exposure_based':
            # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ exposure
            weights = 1 + self.data['exposure'].values
            
        elif method == 'visit_based':
            # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ visit
            weights = 1 + self.data['visit'].values
            
        elif method == 'combined':
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞
            exposure_norm = self.data['exposure'] / (self.data['exposure'].max() + 1e-8)
            visit_norm = self.data['visit'] / (self.data['visit'].max() + 1e-8)
            weights = 1 + 0.5 * (exposure_norm + visit_norm)
            
        elif method == 'inverse_variance':
            # –û–±—Ä–∞—Ç–Ω—ã–µ –≤–µ—Å–∞ –¥–∏—Å–ø–µ—Ä—Å–∏–∏
            network_score = self.data['exposure'] + self.data['visit']
            network_var = np.var(network_score)
            if network_var > 0:
                weights = 1 / (1 + network_var * np.abs(network_score))
            else:
                weights = np.ones(len(self.data))
            
        else:
            weights = np.ones(len(self.data))
            
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        weights = weights / (weights.sum() + 1e-8) * len(weights)
        return weights
    # ..
    def pylift_transformed_outcome_analysis(self, sample_size: int = None) -> Dict:# origin
        """
        –ê–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Transformed Outcome –ø–æ–¥—Ö–æ–¥–∞ –∏–∑ PyLift
        """
        if not PYLIFT_AVAILABLE:
            return self._alternative_transformed_outcome_analysis(sample_size)
        
        print("–ó–∞–ø—É—Å–∫ PyLift Transformed Outcome –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if sample_size and len(self.data) > sample_size:
            sample_data = self.data.sample(n=sample_size, random_state=42)
            print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞: {len(sample_data):,} –∏–∑ {len(self.data):,}")
        else:
            sample_data = self.data
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π —Å —Å–µ—Ç–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        X_features = sample_data[self.feature_cols + self.network_vars].copy()
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        X_features = X_features.fillna(X_features.median())
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ Transformed Outcome –º–æ–¥–µ–ª–∏
            to_model = TransformedOutcome(
                df=sample_data,
                col_treatment=self.treatment_col,
                col_outcome=self.outcome_col,
                col_features=list(X_features.columns)
            )
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            to_model.fit()
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π uplift
            uplift_predictions = to_model.predict(X_features)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ ATE –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–≥–æ uplift
            ate_estimate = np.mean(uplift_predictions)
            
            print(f"‚úÖ PyLift Transformed Outcome ATE: {ate_estimate:.6f}")
            
            return {
                'ate_estimate': ate_estimate,
                'uplift_predictions': uplift_predictions,
                'model': to_model,
                'method': 'pylift_transformed_outcome',
                'sample_size': len(sample_data)
            }
            
        except Exception as e:
            return self._alternative_transformed_outcome_analysis(sample_size)
    
    def _alternative_transformed_outcome_analysis(self, sample_size: int = None) -> Dict:# –∞–ª—å—Ç–µ—Ä–Ω
        """
        –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Transformed Outcome –±–µ–∑ PyLift
        """
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if sample_size and len(self.data) > sample_size:
            sample_data = self.data.sample(n=sample_size, random_state=42)
        else:
            sample_data = self.data
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π
        X = sample_data[self.feature_cols + self.network_vars].fillna(0)
        treatment = sample_data[self.treatment_col]
        outcome = sample_data[self.outcome_col]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ transformed outcome
        # TO = Y * T / p(T=1|X) - Y * (1-T) / p(T=0|X)
        
        # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏ —Å–∫–æ—Ä–æ–≤
        prop_model = LogisticRegression(random_state=42, max_iter=1000)
        prop_model.fit(X, treatment)
        propensity_scores = prop_model.predict_proba(X)[:, 1]
        propensity_scores = np.clip(propensity_scores, 0.01, 0.99)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ transformed outcome
        transformed_outcome = (
            outcome * treatment / propensity_scores - 
            outcome * (1 - treatment) / (1 - propensity_scores)
        )
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è transformed outcome
        to_model = RandomForestRegressor(n_estimators=100, random_state=42)
        to_model.fit(X, transformed_outcome)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è uplift
        uplift_predictions = to_model.predict(X)
        
        # ATE –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ uplift
        ate_estimate = np.mean(uplift_predictions)
        
        print(f"Transformed Outcome ATE: {ate_estimate:.6f}")
        
        return {
            'ate_estimate': ate_estimate,
            'uplift_predictions': uplift_predictions,
            'transformed_outcome': transformed_outcome,
            'propensity_model': prop_model,
            'uplift_model': to_model,
            'method': 'alternative_transformed_outcome',
            'sample_size': len(sample_data)
        }
    
    def pylift_evaluation_metrics(self, uplift_predictions: np.ndarray) -> Dict:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyLift
        """
        if not PYLIFT_AVAILABLE:
            return self._alternative_evaluation_metrics(uplift_predictions)
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            eval_data = self.data.copy()
            eval_data['uplift_pred'] = uplift_predictions
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            evaluator = UpliftEval(
                treatment_col=self.treatment_col,
                outcome_col=self.outcome_col,
                prediction_col='uplift_pred'
            )
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            metrics = evaluator.evaluate(eval_data)
            
            return {
                'pylift_metrics': metrics,
                'method': 'pylift_evaluation'
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ PyLift –æ—Ü–µ–Ω–∫–µ: {e}")
            return self._alternative_evaluation_metrics(uplift_predictions)
    
    def _alternative_evaluation_metrics(self, uplift_predictions: np.ndarray) -> Dict:
        """
        –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        treatment = self.data[self.treatment_col].values
        outcome = self.data[self.outcome_col].values
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –¥–µ–∫–∏–ª–∏ –ø–æ uplift
        n_deciles = 10
        deciles = pd.qcut(uplift_predictions, q=n_deciles, labels=False, duplicates='drop')
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –¥–µ–∫–∏–ª—è–º
        decile_metrics = []
        for i in range(n_deciles):
            mask = deciles == i
            if mask.sum() > 0:
                treated_mask = mask & (treatment == 1)
                control_mask = mask & (treatment == 0)
                
                if treated_mask.sum() > 0 and control_mask.sum() > 0:
                    treated_rate = outcome[treated_mask].mean()
                    control_rate = outcome[control_mask].mean()
                    lift = treated_rate - control_rate
                    
                    decile_metrics.append({
                        'decile': i,
                        'size': mask.sum(),
                        'treated_rate': treated_rate,
                        'control_rate': control_rate,
                        'lift': lift
                    })
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        overall_metrics = {
            'mean_uplift_prediction': np.mean(uplift_predictions),
            'std_uplift_prediction': np.std(uplift_predictions),
            'decile_metrics': decile_metrics
        }
        
        return {
            'alternative_metrics': overall_metrics,
            'method': 'alternative_evaluation'
        }
    
    def network_weighted_pylift_analysis(self, sample_size: int = None) -> Dict:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Å–µ—Ç–µ–≤—ã–º–∏ –≤–µ—Å–∞–º–∏ –∏ PyLift
        """
        print("üï∏Ô∏è  –ó–∞–ø—É—Å–∫ Network-Weighted PyLift –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if sample_size and len(self.data) > sample_size:
            sample_data = self.data.sample(n=sample_size, random_state=42)
        else:
            sample_data = self.data
        
        results = {}
        
        # 1. –ë–∞–∑–æ–≤—ã–π PyLift –∞–Ω–∞–ª–∏–∑
        pylift_result = self.pylift_transformed_outcome_analysis(sample_size)
        results['base_pylift'] = pylift_result
        
        # 2. –ê–Ω–∞–ª–∏–∑ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–µ—Ç–µ–≤—ã–º–∏ –≤–µ—Å–∞–º–∏
        weight_methods = ['exposure_based', 'visit_based', 'combined']
        
        for method in weight_methods:
            print(f"   –ê–Ω–∞–ª–∏–∑ —Å –≤–µ—Å–∞–º–∏: {method}")
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            weights = self.calculate_network_weights(method)
            
            if sample_size and len(self.data) > sample_size:
                weights = weights[:len(sample_data)]
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ATE
            treatment = sample_data[self.treatment_col].values
            outcome = sample_data[self.outcome_col].values
            
            treated_outcomes = outcome[treatment == 1]
            control_outcomes = outcome[treatment == 0]
            treated_weights = weights[treatment == 1]
            control_weights = weights[treatment == 0]
            
            if len(treated_outcomes) > 0 and len(control_outcomes) > 0:
                weighted_ate = (
                    np.average(treated_outcomes, weights=treated_weights) - 
                    np.average(control_outcomes, weights=control_weights)
                )
                
                results[f'weighted_{method}'] = {
                    'ate_estimate': weighted_ate,
                    'weights': weights,
                    'method': f'network_weighted_{method}'
                }
        
        # 3. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        all_ates = [result['ate_estimate'] for result in results.values() 
                   if 'ate_estimate' in result]
        
        if all_ates:
            combined_ate = np.mean(all_ates)
            ate_std = np.std(all_ates)
            
            results['combined'] = {
                'ate_estimate': combined_ate,
                'ate_std': ate_std,
                'individual_estimates': all_ates,
                'method': 'combined_network_pylift'
            }
            
            print(f"‚úÖ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ATE: {combined_ate:.6f} ¬± {ate_std:.6f}")
        
        return results

class NetworkInstrumentalVariablesPyLift:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Network Instrumental Variables (NIV) —Å PyLift
    """
    
    def __init__(self, data: pd.DataFrame, treatment_col: str = 'treatment', 
                 outcome_col: str = 'conversion', network_vars: List[str] = ['visit', 'exposure']):
        self.data = data.copy()
        self.treatment_col = treatment_col
        self.outcome_col = outcome_col
        self.network_vars = network_vars
        self.feature_cols = [col for col in data.columns 
                           if col.startswith('f') and col not in [treatment_col, outcome_col] + network_vars]
        
        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NIV –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"   –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: {self.network_vars}")
        print(f"   –§–∏—á–∏: {len(self.feature_cols)}")
    
    def check_instrument_validity(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        results = {}
        
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤...")
        
        for iv in self.network_vars:
            print(f"   –ê–Ω–∞–ª–∏–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {iv}")
            
            # 1. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è IV —Å treatment
            relevance = self.data[iv].corr(self.data[self.treatment_col])
            
            # 2. –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å: —á–∞—Å—Ç–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è IV —Å outcome –ø—Ä–∏ –∫–æ–Ω—Ç—Ä–æ–ª–µ treatment
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å—Ç–∞—Ç–∫–∏ –æ—Ç —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ outcome –Ω–∞ treatment
            X_treatment = add_constant(self.data[self.treatment_col])
            outcome_model = OLS(self.data[self.outcome_col], X_treatment).fit()
            outcome_residuals = outcome_model.resid
            
            exclusivity = self.data[iv].corr(outcome_residuals)
            
            # 3. F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            X_iv = add_constant(self.data[iv])
            first_stage = OLS(self.data[self.treatment_col], X_iv).fit()
            f_stat = first_stage.fvalue
            
            # 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
            iv_treatment_r2 = first_stage.rsquared
            
            results[iv] = {
                'relevance': relevance,
                'exclusivity': abs(exclusivity),
                'f_statistic': f_stat,
                'first_stage_r2': iv_treatment_r2,
                'weak_instrument': f_stat < 10,  # –ü—Ä–∞–≤–∏–ª–æ Staiger-Stock
                'first_stage_model': first_stage
            }
            
            print(f"      –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.4f}")
            print(f"      –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å: {abs(exclusivity):.4f}")
            print(f"      F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {f_stat:.2f}")
            print(f"      –°–ª–∞–±—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {'–î–∞' if f_stat < 10 else '–ù–µ—Ç'}")
        
        return results
    
    def two_stage_least_squares(self, instrument: str, sample_size: int = None) -> Dict:
        """
        –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ (2SLS)
        """
        print(f"üéØ 2SLS –∞–Ω–∞–ª–∏–∑ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º: {instrument}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if sample_size and len(self.data) > sample_size:
            sample_data = self.data.sample(n=sample_size, random_state=42)
        else:
            sample_data = self.data
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        X_controls = sample_data[self.feature_cols].fillna(0)
        instrument_var = sample_data[instrument]
        treatment = sample_data[self.treatment_col]
        outcome = sample_data[self.outcome_col]
        
        # –≠—Ç–∞–ø 1: –†–µ–≥—Ä–µ—Å—Å–∏—è treatment –Ω–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏
        X_first_stage = add_constant(pd.concat([instrument_var, X_controls], axis=1))
        first_stage = OLS(treatment, X_first_stage).fit()
        treatment_fitted = first_stage.fittedvalues
        
        # –≠—Ç–∞–ø 2: –†–µ–≥—Ä–µ—Å—Å–∏—è outcome –Ω–∞ fitted treatment –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏
        X_second_stage = add_constant(pd.concat([treatment_fitted, X_controls], axis=1))
        second_stage = OLS(outcome, X_second_stage).fit()
        
        # ATE - —ç—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–∏ treatment –≤ –≤—Ç–æ—Ä–æ–π —Å—Ç–∞–¥–∏–∏
        ate_estimate = second_stage.params[1]  # –ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –ø–æ—Å–ª–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
        ate_se = second_stage.bse[1]
        
        print(f"   2SLS ATE: {ate_estimate:.6f} (SE: {ate_se:.6f})")
        
        return {
            'ate_estimate': ate_estimate,
            'ate_se': ate_se,
            'first_stage_model': first_stage,
            'second_stage_model': second_stage,
            'first_stage_r2': first_stage.rsquared,
            'second_stage_r2': second_stage.rsquared,
            'instrument': instrument,
            'method': '2sls',
            'sample_size': len(sample_data)
        }
    
    def pylift_instrumental_analysis(self, sample_size: int = None) -> Dict:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ NIV —Å PyLift –º–µ—Ç–æ–¥–∞–º–∏
        """
        print("üî¨ PyLift Instrumental Variables –∞–Ω–∞–ª–∏–∑...")
        
        results = {}
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        iv_validity = self.check_instrument_validity()
        results['instrument_validity'] = iv_validity
        
        # 2. 2SLS –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        for instrument in self.network_vars:
            if not iv_validity[instrument]['weak_instrument']:
                tsls_result = self.two_stage_least_squares(instrument, sample_size)
                results[f'2sls_{instrument}'] = tsls_result
            else:
                print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫ —Å–ª–∞–±–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {instrument}")
        
        # 3. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —á–µ—Ä–µ–∑ PyLift
        if PYLIFT_AVAILABLE:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏
                pylift_analyzer = PyLiftNetworkAnalyzer(
                    self.data, self.treatment_col, self.outcome_col, self.network_vars
                )
                
                pylift_result = pylift_analyzer.pylift_transformed_outcome_analysis(sample_size)
                results['pylift_instrumental'] = pylift_result
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ PyLift instrumental –∞–Ω–∞–ª–∏–∑–µ: {e}")
        
        return results

def comprehensive_pylift_ate_analysis(data: pd.DataFrame, sample_size: int = None) -> Dict:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ATE —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyLift –∏ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    """
    print("üöÄ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô PYLIFT ATE –ê–ù–ê–õ–ò–ó")
    print("=" * 50)
    
    results = {}
    
    # 1. –ë–∞–∑–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏
    print("üìä –ë–∞–∑–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏...")
    naive_ate = (data[data['treatment'] == 1]['conversion'].mean() - 
                data[data['treatment'] == 0]['conversion'].mean())
    results['naive_ate'] = naive_ate
    print(f"   –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ATE: {naive_ate:.6f}")
    
    # 2. PyLift Network –∞–Ω–∞–ª–∏–∑
    print("\nüï∏Ô∏è  PyLift Network –∞–Ω–∞–ª–∏–∑...")
    network_analyzer = PyLiftNetworkAnalyzer(data)
    pylift_results = network_analyzer.network_weighted_pylift_analysis(sample_size)
    results['pylift_network'] = pylift_results
    
    # 3. NIV –∞–Ω–∞–ª–∏–∑ —Å PyLift
    print("\nüéØ Network Instrumental Variables –∞–Ω–∞–ª–∏–∑...")
    niv_analyzer = NetworkInstrumentalVariablesPyLift(data)
    niv_results = niv_analyzer.pylift_instrumental_analysis(sample_size)
    results['niv_analysis'] = niv_results
    
    # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
    print("\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤:")
    all_estimates = {'–ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞': naive_ate}
    
    # –î–æ–±–∞–≤–ª—è–µ–º PyLift –æ—Ü–µ–Ω–∫–∏
    if 'combined' in pylift_results:
        all_estimates['PyLift Combined'] = pylift_results['combined']['ate_estimate']
    
    if 'base_pylift' in pylift_results:
        all_estimates['PyLift Base'] = pylift_results['base_pylift']['ate_estimate']
    
    # –î–æ–±–∞–≤–ª—è–µ–º 2SLS –æ—Ü–µ–Ω–∫–∏
    for key, value in niv_results.items():
        if key.startswith('2sls_') and 'ate_estimate' in value:
            instrument = key.replace('2sls_', '')
            all_estimates[f'2SLS ({instrument})'] = value['ate_estimate']
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    for method, estimate in all_estimates.items():
        print(f"   {method:20}: {estimate:8.6f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if len(all_estimates) > 1:
        estimates_values = list(all_estimates.values())
        mean_estimate = np.mean(estimates_values)
        std_estimate = np.std(estimates_values)
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {mean_estimate:.6f}")
        print(f"   Std: {std_estimate:.6f}")
        print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: {np.max(estimates_values) - np.min(estimates_values):.6f}")
        
        results['summary'] = {
            'all_estimates': all_estimates,
            'mean_estimate': mean_estimate,
            'std_estimate': std_estimate,
            'range': np.max(estimates_values) - np.min(estimates_values)
        }
    
    # 5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    
    if 'pylift_network' in results and 'combined' in results['pylift_network']:
        recommended_ate = results['pylift_network']['combined']['ate_estimate']
        recommended_std = results['pylift_network']['combined']['ate_std']
        
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –æ—Ü–µ–Ω–∫–∞ ATE: {recommended_ate:.6f} ¬± {recommended_std:.6f}")
        print(f"   –ú–µ—Ç–æ–¥: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π PyLift —Å —Å–µ—Ç–µ–≤—ã–º–∏ –≤–µ—Å–∞–º–∏")
        
        results['recommendation'] = {
            'ate_estimate': recommended_ate,
            'ate_std': recommended_std,
            'method': 'Combined PyLift Network-Weighted'
        }
    
    return results

def quick_pylift_analysis(data: pd.DataFrame, sample_size: int = 50000) -> Dict:
    """
    –ë—ã—Å—Ç—Ä—ã–π PyLift –∞–Ω–∞–ª–∏–∑ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    """
    print("‚ö° –ë–´–°–¢–†–´–ô PYLIFT –ê–ù–ê–õ–ò–ó")
    print("=" * 30)
    
    # 1. –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    naive_ate = (data[data['treatment'] == 1]['conversion'].mean() - 
                data[data['treatment'] == 0]['conversion'].mean())
    print(f"üìä –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {naive_ate:.6f}")
    
    # 2. –ë—ã—Å—Ç—Ä—ã–π PyLift –∞–Ω–∞–ª–∏–∑
    analyzer = PyLiftNetworkAnalyzer(data)
    
    # Transformed Outcome
    to_result = analyzer.pylift_transformed_outcome_analysis(sample_size)
    
    # Network-weighted –∞–Ω–∞–ª–∏–∑
    network_result = analyzer.network_weighted_pylift_analysis(sample_size)
    
    results = {
        'naive_ate': naive_ate,
        'transformed_outcome': to_result['ate_estimate'],
        'network_weighted': network_result.get('combined', {}).get('ate_estimate', 'N/A')
    }
    
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    for method, estimate in results.items():
        if isinstance(estimate, (int, float)):
            print(f"   {method}: {estimate:.6f}")
        else:
            print(f"   {method}: {estimate}")
    
    return {
        'quick_estimates': results,
        'detailed_results': {
            'transformed_outcome': to_result,
            'network_weighted': network_result
        }
    }

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞—é—Ç—Å—è —Ç–µ–º–∏ –∂–µ
def network_effects_diagnostics(data: pd.DataFrame) -> Dict:
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
    
    print("\nüî¨ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ï–¢–ï–í–´–• –≠–§–§–ï–ö–¢–û–í")
    print("=" * 40)
    
    diagnostics = {}
    
    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    network_vars = ['visit', 'exposure']
    corr_matrix = data[['treatment', 'conversion'] + network_vars].corr()
    
    print("üìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞:")
    print(corr_matrix.round(4))
    
    diagnostics['correlation_matrix'] = corr_matrix
    
    return diagnostics


class NetworkWeightedOutcomeEstimation:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Network-based Weighted Outcome Estimation (NWOE)
    """
    
    def __init__(self, data: pd.DataFrame, treatment_col: str = 'treatment', 
                 outcome_col: str = 'conversion', network_vars: List[str] = ['visit', 'exposure']):
        self.data = data.copy()
        self.treatment_col = treatment_col
        self.outcome_col = outcome_col
        self.network_vars = network_vars
        self.feature_cols = [col for col in data.columns 
                           if col.startswith('f') and col not in [treatment_col, outcome_col] + network_vars]
        
    def calculate_network_weights(self, method: str = 'exposure_based') -> np.ndarray:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ—Ç–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        
        if method == 'exposure_based':
            # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ exposure: –±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è —É–∑–ª–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º exposure
            weights = 1 + self.data['exposure'].values
            
        elif method == 'visit_based':
            # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ visit: –±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è —É–∑–ª–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º visit
            weights = 1 + self.data['visit'].values
            
        elif method == 'combined':
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞
            exposure_norm = self.data['exposure'] / self.data['exposure'].max()
            visit_norm = self.data['visit'] / self.data['visit'].max()
            weights = 1 + 0.5 * (exposure_norm + visit_norm)
            
        elif method == 'inverse_variance':
            # –û–±—Ä–∞—Ç–Ω—ã–µ –≤–µ—Å–∞ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ (–¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤)
            network_score = self.data['exposure'] + self.data['visit']
            weights = 1 / (1 + np.var(network_score) * network_score)
            
        else:
            weights = np.ones(len(self.data))
            
        return weights / weights.sum()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    
    def propensity_score_weighting(self, use_network: bool = True) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä–æ–≤ —Å —É—á–µ—Ç–æ–º —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-–º–æ–¥–µ–ª–∏
        if use_network:
            X_prop = np.hstack([
                self.data[self.feature_cols].values,
                self.data[self.network_vars].values
            ])
        else:
            X_prop = self.data[self.feature_cols].values
            
        # –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-–º–æ–¥–µ–ª–∏
        prop_model = LogisticRegression(random_state=42, max_iter=1000)
        prop_model.fit(X_prop, self.data[self.treatment_col])
        propensity_scores = prop_model.predict_proba(X_prop)[:, 1]
        
        # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤
        propensity_scores = np.clip(propensity_scores, 0.01, 0.99)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IPW –≤–µ—Å–æ–≤
        treatment = self.data[self.treatment_col].values
        ipw_weights = treatment / propensity_scores + (1 - treatment) / (1 - propensity_scores)
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Ç–µ–≤—ã–µ –≤–µ—Å–∞, –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º –∏—Ö —Å IPW
        if use_network:
            network_weights = self.calculate_network_weights('combined')
            final_weights = ipw_weights * network_weights * len(self.data)
        else:
            final_weights = ipw_weights
            
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        final_weights = final_weights / final_weights.sum() * len(self.data)
        
        # –û—Ü–µ–Ω–∫–∞ ATE —Å –≤–µ—Å–∞–º–∏
        outcome = self.data[self.outcome_col].values
        treated_outcomes = outcome[treatment == 1]
        control_outcomes = outcome[treatment == 0]
        treated_weights = final_weights[treatment == 1]
        control_weights = final_weights[treatment == 0]
        
        ate_estimate = (np.average(treated_outcomes, weights=treated_weights) - 
                       np.average(control_outcomes, weights=control_weights))
        
        return {
            'ate_estimate': ate_estimate,
            'propensity_scores': propensity_scores,
            'weights': final_weights,
            'model': prop_model
        }
    
    def doubly_robust_estimation(self, sample_size: int = None, use_simple_models: bool = True) -> Dict:
        """–ë—ã—Å—Ç—Ä–∞—è –¥–≤–∞–∂–¥—ã —Ä–æ–±–∞—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å —Å–µ—Ç–µ–≤—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏"""
        
        print("–ó–∞–ø—É—Å–∫ Doubly Robust –æ—Ü–µ–Ω–∫–∏...")
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if sample_size and len(self.data) > sample_size:
            print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–º {sample_size:,} –∏–∑ {len(self.data):,}")
            sample_data = self.data.sample(n=sample_size, random_state=42)
        else:
            sample_data = self.data
            
        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_features = sample_data[self.feature_cols].values
        X_network = sample_data[self.network_vars].values
        X_prop = np.hstack([X_features, X_network])
        
        treatment = sample_data[self.treatment_col].values
        outcome = sample_data[self.outcome_col].values
        
        print("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-–º–æ–¥–µ–ª–∏...")
        # 1. –ü—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä—ã
        prop_model = LogisticRegression(random_state=42, max_iter=500, solver='lbfgs')
        prop_model.fit(X_prop, treatment)
        propensity_scores = prop_model.predict_proba(X_prop)[:, 1]
        propensity_scores = np.clip(propensity_scores, 0.01, 0.99)
        
        print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏—Å—Ö–æ–¥–æ–≤...")
        # 2. –ú–æ–¥–µ–ª–∏ –∏—Å—Ö–æ–¥–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        control_mask = treatment == 0
        treated_mask = treatment == 1
        
        if use_simple_models:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LinearRegression –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            control_model = LinearRegression()
            treated_model = LinearRegression()
        else:
            # RandomForest —Å –º–µ–Ω—å—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            control_model = RandomForestRegressor(n_estimators=50, max_depth=10, 
                                                n_jobs=-1, random_state=42)
            treated_model = RandomForestRegressor(n_estimators=50, max_depth=10, 
                                                n_jobs=-1, random_state=42)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        control_model.fit(X_prop[control_mask], outcome[control_mask])
        treated_model.fit(X_prop[treated_mask], outcome[treated_mask])
        
        print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        # 3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        mu0_hat = control_model.predict(X_prop)
        mu1_hat = treated_model.predict(X_prop)
        
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ ATE...")
        # 4. –î–≤–∞–∂–¥—ã —Ä–æ–±–∞—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        ipw_term1 = treatment * (outcome - mu1_hat) / propensity_scores
        ipw_term2 = (1 - treatment) * (outcome - mu0_hat) / (1 - propensity_scores)
        
        ate_estimate = np.mean(mu1_hat - mu0_hat) + np.mean(ipw_term1) - np.mean(ipw_term2)
        
        print("–û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
        return {
            'ate_estimate': ate_estimate,
            'mu0_predictions': mu0_hat,
            'mu1_predictions': mu1_hat,
            'propensity_scores': propensity_scores,
            'control_model': control_model,
            'treated_model': treated_model,
            'sample_size_used': len(sample_data)
        }
    
    def fast_nwoe_estimation(self, sample_size: int = 100000) -> Dict:
        """–ë—ã—Å—Ç—Ä–∞—è NWOE –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        
        print(f"‚ö° –ë—ã—Å—Ç—Ä–∞—è NWOE –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—ã–±–æ—Ä–∫–µ {sample_size:,}...")
        
        # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
        sample_data = self.data.groupby('treatment').apply(
            lambda x: x.sample(n=min(len(x), sample_size//2), random_state=42)
        ).reset_index(drop=True)
        
        print(f"üìä –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {len(sample_data):,}")
        print(f"   Treatment 0: {(sample_data['treatment']==0).sum():,}")
        print(f"   Treatment 1: {(sample_data['treatment']==1).sum():,}")
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-–º–æ–¥–µ–ª—å
        X_simple = sample_data[self.feature_cols[:6] + self.network_vars].values  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 6 —Ñ–∏—á–µ–π
        
        prop_model = LogisticRegression(random_state=42, max_iter=300)
        prop_model.fit(X_simple, sample_data[self.treatment_col])
        propensity_scores = prop_model.predict_proba(X_simple)[:, 1]
        propensity_scores = np.clip(propensity_scores, 0.05, 0.95)
        
        # –°–µ—Ç–µ–≤—ã–µ –≤–µ—Å–∞
        network_weights = 1 + 0.5 * (sample_data['exposure'] + sample_data['visit'])
        network_weights = network_weights / network_weights.sum() * len(sample_data)
        
        # IPW –≤–µ—Å–∞
        treatment = sample_data[self.treatment_col].values
        ipw_weights = treatment / propensity_scores + (1 - treatment) / (1 - propensity_scores)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞
        final_weights = ipw_weights * network_weights
        final_weights = np.clip(final_weights, 0.1, 10)  # –û–±—Ä–µ–∑–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤
        
        # ATE –æ—Ü–µ–Ω–∫–∞
        outcome = sample_data[self.outcome_col].values
        treated_outcomes = outcome[treatment == 1]
        control_outcomes = outcome[treatment == 0]
        treated_weights = final_weights[treatment == 1]
        control_weights = final_weights[treatment == 0]
        
        ate_estimate = (np.average(treated_outcomes, weights=treated_weights) - 
                       np.average(control_outcomes, weights=control_weights))
        
        print(f"‚úÖ –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: ATE = {ate_estimate:.6f}")
        
        return {
            'ate_estimate': ate_estimate,
            'propensity_scores': propensity_scores,
            'weights': final_weights,
            'sample_size': len(sample_data),
            'method': 'fast_nwoe'
        }

def comprehensive_ate_analysis(data: pd.DataFrame) -> Dict:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ATE —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
    
    print("üîç –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê ATE –° –°–ï–¢–ï–í–´–ú–ò –≠–§–§–ï–ö–¢–ê–ú–ò")
    print("=" * 60)
    
    results = {}
    
    # 1. –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–Ω–∏—Ö (–Ω–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
    naive_ate = (data[data['treatment'] == 1]['conversion'].mean() - 
                data[data['treatment'] == 0]['conversion'].mean())
    results['naive_ate'] = naive_ate
    
    print(f"üìä –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ATE: {naive_ate:.6f}")
    print()
    
    # 2. –ê–Ω–∞–ª–∏–∑ NIV
    print("üîß –ê–ù–ê–õ–ò–ó NETWORK INSTRUMENTAL VARIABLES (NIV)")
    print("-" * 40)
    
    niv_analyzer = NetworkInstrumentalVariables(data)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    iv_validity = niv_analyzer.check_instrument_validity()
    
    for iv, validity in iv_validity.items():
        print(f"üìà –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç '{iv}':")
        print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (–∫–æ—Ä—Ä. —Å treatment): {validity['relevance']:.4f}")
        print(f"   –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å (–æ—Å—Ç–∞—Ç. –∫–æ—Ä—Ä. —Å outcome): {validity['exclusivity']:.4f}")
        print(f"   F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {validity['f_statistic']:.2f}")
        print(f"   –°–ª–∞–±—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {'‚ö†Ô∏è  –î–∞' if validity['weak_instrument'] else '‚úÖ –ù–µ—Ç'}")
        print()
    
    # 2SLS –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    niv_estimates = {}
    for iv in ['visit', 'exposure']:
        tsls_result = niv_analyzer.two_stage_least_squares(iv)
        niv_estimates[iv] = tsls_result
        print(f"üéØ 2SLS –æ—Ü–µ–Ω–∫–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º '{iv}': {tsls_result['ate_estimate']:.6f}")
        print(f"   R¬≤ –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞: {tsls_result['first_stage_r2']:.4f}")
        print()
    
    results['niv_estimates'] = niv_estimates
    results['iv_validity'] = iv_validity
    
    # 3. –ê–Ω–∞–ª–∏–∑ NWOE
    print("üï∏Ô∏è  –ê–ù–ê–õ–ò–ó NETWORK-BASED WEIGHTED OUTCOME ESTIMATION (NWOE)")
    print("-" * 50)
    
    nwoe_analyzer = NetworkWeightedOutcomeEstimation(data)
    
    # –ü—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –±–µ–∑ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    psw_standard = nwoe_analyzer.propensity_score_weighting(use_network=False)
    print(f"‚öñÔ∏è  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ PSW: {psw_standard['ate_estimate']:.6f}")
    
    # –ü—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å —Å–µ—Ç–µ–≤—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏
    psw_network = nwoe_analyzer.propensity_score_weighting(use_network=True)
    print(f"üï∏Ô∏è  Network-enhanced PSW: {psw_network['ate_estimate']:.6f}")
    
    # –î–≤–∞–∂–¥—ã —Ä–æ–±–∞—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    dr_result = nwoe_analyzer.doubly_robust_estimation()
    print(f"üõ°Ô∏è  Doubly Robust: {dr_result['ate_estimate']:.6f}")
    print()
    
    results['nwoe_estimates'] = {
        'psw_standard': psw_standard,
        'psw_network': psw_network,
        'doubly_robust': dr_result
    }
    
    # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
    print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ú–ï–¢–û–î–û–í")
    print("-" * 30)
    
    all_estimates = {
        '–ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞': naive_ate,
        '2SLS (visit)': niv_estimates['visit']['ate_estimate'],
        '2SLS (exposure)': niv_estimates['exposure']['ate_estimate'],
        '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ PSW': psw_standard['ate_estimate'],
        'Network PSW': psw_network['ate_estimate'],
        'Doubly Robust': dr_result['ate_estimate']
    }
    
    for method, estimate in all_estimates.items():
        print(f"{method:15}: {estimate:8.6f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–∑–±—Ä–æ—Å–∞
    estimates_values = list(all_estimates.values())
    print()
    print(f"üìà –°—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –º–µ—Ç–æ–¥–∞–º: {np.mean(estimates_values):.6f}")
    print(f"üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(estimates_values):.6f}")
    print(f"üìè –†–∞–∑–º–∞—Ö: {np.max(estimates_values) - np.min(estimates_values):.6f}")
    
    results['all_estimates'] = all_estimates
    results['summary_stats'] = {
        'mean': np.mean(estimates_values),
        'std': np.std(estimates_values),
        'range': np.max(estimates_values) - np.min(estimates_values)
    }
    
    return results

def network_effects_diagnostics(data: pd.DataFrame) -> Dict:
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
    
    print("\nüî¨ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ï–¢–ï–í–´–• –≠–§–§–ï–ö–¢–û–í")
    print("=" * 40)
    
    diagnostics = {}
    
    # 1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    network_vars = ['visit', 'exposure']
    corr_matrix = data[['treatment', 'conversion'] + network_vars].corr()
    
    print("üìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞:")
    print(corr_matrix.round(4))
    print()
    
    # 2. –ê–Ω–∞–ª–∏–∑ spillover —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    print("üåä –ê–ù–ê–õ–ò–ó SPILLOVER –≠–§–§–ï–ö–¢–û–í")
    print("-" * 30)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º network exposure
    data['exposure_quartile'] = pd.qcut(data['exposure'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
    data['visit_quartile'] = pd.qcut(data['visit'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º exposure
    exposure_analysis = data.groupby(['exposure_quartile', 'treatment']).agg({
        'conversion': ['count', 'mean', 'std']
    }).round(6)
    
    print("üìà –ö–æ–Ω–≤–µ—Ä—Å–∏—è –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º exposure:")
    print(exposure_analysis)
    print()
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º visit
    visit_analysis = data.groupby(['visit_quartile', 'treatment']).agg({
        'conversion': ['count', 'mean', 'std']
    }).round(6)
    
    print("üìà –ö–æ–Ω–≤–µ—Ä—Å–∏—è –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º visit:")
    print(visit_analysis)
    print()
    
    # 3. –¢–µ—Å—Ç –Ω–∞ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    print("üîç –¢–ï–°–¢ –ù–ê –ì–ï–¢–ï–†–û–ì–ï–ù–ù–û–°–¢–¨ –≠–§–§–ï–ö–¢–û–í")
    print("-" * 35)
    
    # ATE –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º exposure
    ate_by_exposure = {}
    for q in ['Q1', 'Q2', 'Q3', 'Q4']:
        subset = data[data['exposure_quartile'] == q]
        ate_q = (subset[subset['treatment'] == 1]['conversion'].mean() - 
                subset[subset['treatment'] == 0]['conversion'].mean())
        ate_by_exposure[q] = ate_q
        print(f"ATE –≤ {q} –∫–≤–∞—Ä—Ç–∏–ª–µ exposure: {ate_q:.6f}")
    
    print()
    
    # ATE –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º visit
    ate_by_visit = {}
    for q in ['Q1', 'Q2', 'Q3', 'Q4']:
        subset = data[data['visit_quartile'] == q]
        ate_q = (subset[subset['treatment'] == 1]['conversion'].mean() - 
                subset[subset['treatment'] == 0]['conversion'].mean())
        ate_by_visit[q] = ate_q
        print(f"ATE –≤ {q} –∫–≤–∞—Ä—Ç–∏–ª–µ visit: {ate_q:.6f}")
    
    diagnostics['correlation_matrix'] = corr_matrix
    diagnostics['ate_by_exposure'] = ate_by_exposure  
    diagnostics['ate_by_visit'] = ate_by_visit
    diagnostics['exposure_analysis'] = exposure_analysis
    diagnostics['visit_analysis'] = visit_analysis
    
    return diagnostics

def create_visualization_plots(data: pd.DataFrame, results: Dict):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('–ê–Ω–∞–ª–∏–∑ ATE —Å —Å–µ—Ç–µ–≤—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏', fontsize=16, fontweight='bold')
    
    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ ATE
    methods = list(results['all_estimates'].keys())
    estimates = list(results['all_estimates'].values())
    
    axes[0, 0].barh(methods, estimates, color='skyblue', alpha=0.7)
    axes[0, 0].set_xlabel('ATE Estimate')
    axes[0, 0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ ATE')
    axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)
    
    # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    axes[0, 1].hist(data['visit'], bins=50, alpha=0.7, label='Visit', density=True)
    axes[0, 1].hist(data['exposure'], bins=50, alpha=0.7, label='Exposure', density=True)
    axes[0, 1].set_xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
    axes[0, 1].set_ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å')
    axes[0, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö')
    axes[0, 1].legend()
    
    # 3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
    network_corr = data[['treatment', 'conversion', 'visit', 'exposure']].corr()
    sns.heatmap(network_corr, annot=True, cmap='coolwarm', center=0, 
                ax=axes[0, 2], square=True)
    axes[0, 2].set_title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞')
    
    # 4. –ü—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä—ã
    psw_result = results['nwoe_estimates']['psw_network']
    axes[1, 0].hist(psw_result['propensity_scores'], bins=50, alpha=0.7, 
                   color='lightgreen', edgecolor='black')
    axes[1, 0].set_xlabel('Propensity Score')
    axes[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä–æ–≤')
    
    # 5. Conversion rate –ø–æ –≥—Ä—É–ø–ø–∞–º –∏ —Å–µ—Ç–µ–≤—ã–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
    data['exposure_bin'] = pd.cut(data['exposure'], bins=10, labels=False)
    grouped_data = data.groupby(['exposure_bin', 'treatment'])['conversion'].mean().reset_index()
    
    for treatment in [0, 1]:
        subset = grouped_data[grouped_data['treatment'] == treatment]
        axes[1, 1].plot(subset['exposure_bin'], subset['conversion'], 
                       marker='o', label=f'Treatment {treatment}')
    
    axes[1, 1].set_xlabel('Exposure Bin')
    axes[1, 1].set_ylabel('Conversion Rate')
    axes[1, 1].set_title('Conversion Rate –ø–æ —É—Ä–æ–≤–Ω—è–º Exposure')
    axes[1, 1].legend()
    
    # 6. –û—Å—Ç–∞—Ç–∫–∏ –º–æ–¥–µ–ª–∏
    dr_result = results['nwoe_estimates']['doubly_robust']
    treatment_mask = data['treatment'] == 1
    
    residuals_treated = (data.loc[treatment_mask, 'conversion'].values - 
                        dr_result['mu1_predictions'][treatment_mask])
    residuals_control = (data.loc[~treatment_mask, 'conversion'].values - 
                        dr_result['mu0_predictions'][~treatment_mask])
    
    axes[1, 2].scatter(dr_result['mu1_predictions'][treatment_mask][:1000], 
                      residuals_treated[:1000], alpha=0.5, label='Treated', s=1)
    axes[1, 2].scatter(dr_result['mu0_predictions'][~treatment_mask][:1000], 
                      residuals_control[:1000], alpha=0.5, label='Control', s=1)
    axes[1, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
    axes[1, 2].set_xlabel('Predicted Values')
    axes[1, 2].set_ylabel('Residuals')
    axes[1, 2].set_title('–û—Å—Ç–∞—Ç–∫–∏ –º–æ–¥–µ–ª–∏ (sample)')
    axes[1, 2].legend()
    
    plt.tight_layout()
    plt.show()
    
    return fig

# –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
def quick_nwoe_analysis(data: pd.DataFrame, sample_size: int = 100000) -> Dict:
    """–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ NWOE –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    print("‚ö° –ë–´–°–¢–†–´–ô NWOE –ê–ù–ê–õ–ò–ó")
    print("=" * 30)
    
    # –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    naive_ate = (data[data['treatment'] == 1]['conversion'].mean() - 
                data[data['treatment'] == 0]['conversion'].mean())
    print(f"üìä –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ATE: {naive_ate:.6f}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NWOE –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    nwoe = NetworkWeightedOutcomeEstimation(data)
    
    # –ë—ã—Å—Ç—Ä—ã–µ –º–µ—Ç–æ–¥—ã
    print("\nüöÄ –ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä—ã—Ö –º–µ—Ç–æ–¥–æ–≤...")
    
    # 1. –ë—ã—Å—Ç—Ä–∞—è NWOE –æ—Ü–µ–Ω–∫–∞
    fast_result = nwoe.fast_nwoe_estimation(sample_size)
    
    # 2. –ë—ã—Å—Ç—Ä–∞—è Doubly Robust —Å –ª–∏–Ω–µ–π–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
    dr_fast = nwoe.doubly_robust_estimation(sample_size, use_simple_models=True)
    
    # 3. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ PSW –Ω–∞ –≤—ã–±–æ—Ä–∫–µ
    sample_data = data.sample(n=min(len(data), sample_size), random_state=42)
    nwoe_sample = NetworkWeightedOutcomeEstimation(sample_data)
    psw_result = nwoe_sample.propensity_score_weighting(use_network=True)
    
    results = {
        'naive_ate': naive_ate,
        'fast_nwoe': fast_result['ate_estimate'],
        'doubly_robust_fast': dr_fast['ate_estimate'],
        'psw_network': psw_result['ate_estimate']
    }
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:     {naive_ate:.6f}")
    print(f"   –ë—ã—Å—Ç—Ä–∞—è NWOE:       {fast_result['ate_estimate']:.6f}")
    print(f"   Doubly Robust:      {dr_fast['ate_estimate']:.6f}")
    print(f"   Network PSW:        {psw_result['ate_estimate']:.6f}")
    
    # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    estimates = list(results.values())
    std_dev = np.std(estimates)
    print(f"\nüìà –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_dev:.6f}")
    
    if std_dev < 0.001:
        print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã")
    else:
        print("‚ö†Ô∏è  –ï—Å—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏")
    
    return {
        'estimates': results,
        'details': {
            'fast_nwoe': fast_result,
            'doubly_robust': dr_fast,
            'psw_network': psw_result
        },
        'consistency': std_dev
    }
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    print("üöÄ –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó NWOE –ò NIV –î–õ–Ø –û–¶–ï–ù–ö–ò ATE")
    print("=" * 50)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
    results = comprehensive_ate_analysis(data)
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    diagnostics = network_effects_diagnostics(data)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    create_visualization_plots(data, results)
    
    # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ò –í–´–í–û–î–´")
    print("=" * 30)
    
    estimates = results['all_estimates']
    std_estimates = results['summary_stats']['std']
    
    if std_estimates < 0.001:
        print("‚úÖ –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏ - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã")
    else:
        print("‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏ - —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    weak_instruments = [iv for iv, validity in results['iv_validity'].items() 
                       if validity['weak_instrument']]
    
    if weak_instruments:
        print(f"üîß –°–ª–∞–±—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã: {weak_instruments}")
        print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ NWOE –º–µ—Ç–æ–¥—ã")
    else:
        print("‚úÖ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–ª—å–Ω—ã –¥–ª—è NIV –∞–Ω–∞–ª–∏–∑–∞")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –ª—É—á—à–µ–º—É –º–µ—Ç–æ–¥—É
    dr_estimate = estimates['Doubly Robust']
    network_psw_estimate = estimates['Network PSW']
    
    print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–ê–Ø –û–¶–ï–ù–ö–ê ATE:")
    print(f"   Doubly Robust: {dr_estimate:.6f}")
    print(f"   (—Å —É—á–µ—Ç–æ–º —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤)")
    
    return {
        'results': results,
        'diagnostics': diagnostics,
        'recommendation': {
            'best_estimate': dr_estimate,
            'method': 'Doubly Robust',
            'confidence': 'high' if std_estimates < 0.001 else 'medium'
        }
    }



# eda
# ..
def comprehensive_eda_for_ate_analysis(df):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π EDA –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ NWOE –∏ NIV –∞–Ω–∞–ª–∏–∑—É ATE
    """
    
    print("="*60)
    print("–ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô EDA –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê ATE (NWOE/NIV)")
    print("="*60)
    
    # 1. –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–ù–ù–´–•
    print("\n1. –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø")
    print("-"*30)
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {len(df)}")
    
    # –í—ã–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    features = [col for col in df.columns if col.startswith('f')]
    treatment_var = 'treatment'
    outcome_var = 'conversion'
    network_vars = ['visit', 'exposure']
    
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π: {len(features)}")
    print(f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è treatment: {treatment_var}")
    print(f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è outcome: {outcome_var}")
    print(f"–°–µ—Ç–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {network_vars}")
    
    # 2. –ê–ù–ê–õ–ò–ó –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø TREATMENT
    print("\n2. –ê–ù–ê–õ–ò–ó TREATMENT –ì–†–£–ü–ü–´")
    print("-"*30)
    treatment_dist = df[treatment_var].value_counts().sort_index()
    treatment_prop = df[treatment_var].value_counts(normalize=True).sort_index()
    
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ treatment:")
    for val, count, prop in zip(treatment_dist.index, treatment_dist.values, treatment_prop.values):
        print(f"  Treatment {val}: {count} ({prop:.3f})")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å
    min_group_size = treatment_dist.min()
    max_group_size = treatment_dist.max()
    imbalance_ratio = max_group_size / min_group_size
    print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {imbalance_ratio:.2f}")
    if imbalance_ratio > 3:
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –≤ –≥—Ä—É–ø–ø–∞—Ö treatment!")
    
    # 3. –ê–ù–ê–õ–ò–ó OUTCOME –ü–ï–†–ï–ú–ï–ù–ù–û–ô
    print("\n3. –ê–ù–ê–õ–ò–ó OUTCOME (CONVERSION)")
    print("-"*30)
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ outcome
    outcome_stats = df[outcome_var].describe()
    print("–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ conversion:")
    print(outcome_stats)
    
    # Outcome –ø–æ –≥—Ä—É–ø–ø–∞–º treatment
    print("\nConversion –ø–æ –≥—Ä—É–ø–ø–∞–º treatment:")
    outcome_by_treatment = df.groupby(treatment_var)[outcome_var].agg(['count', 'mean', 'std'])
    print(outcome_by_treatment)
    
    # –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ outcome –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏
    treatment_groups = [group[outcome_var].values for name, group in df.groupby(treatment_var)]
    if len(treatment_groups) == 2:
        t_stat, p_val = stats.ttest_ind(treatment_groups[0], treatment_groups[1])
        print(f"\nT-test –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏: t={t_stat:.3f}, p={p_val:.4f}")
    
    # 4. –ê–ù–ê–õ–ò–ó –°–ï–¢–ï–í–´–• –ü–ï–†–ï–ú–ï–ù–ù–´–•
    print("\n4. –ê–ù–ê–õ–ò–ó –°–ï–¢–ï–í–´–• –ü–ï–†–ï–ú–ï–ù–ù–´–•")
    print("-"*30)
    
    for var in network_vars:
        print(f"\n{var.upper()}:")
        var_stats = df[var].describe()
        print(f"  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={var_stats['mean']:.3f}, std={var_stats['std']:.3f}")
        print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: [{var_stats['min']:.1f}, {var_stats['max']:.1f}]")
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å treatment –∏ outcome
        corr_treatment = df[var].corr(df[treatment_var])
        corr_outcome = df[var].corr(df[outcome_var])
        print(f"  –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å treatment: {corr_treatment:.3f}")
        print(f"  –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å outcome: {corr_outcome:.3f}")
    
    # 5. –ê–ù–ê–õ–ò–ó –ö–û–í–ê–†–ò–ê–¢ (FEATURES)
    print("\n5. –ê–ù–ê–õ–ò–ó –ö–û–í–ê–†–ò–ê–¢ (FEATURES)")
    print("-"*30)
    
    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏—á–µ–π
    features_stats = df[features].describe()
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ {len(features)} —Ñ–∏—á–∞–º:")
    print(f"  –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: [{features_stats.loc['mean'].min():.3f}, {features_stats.loc['mean'].max():.3f}]")
    print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è: [{features_stats.loc['std'].min():.3f}, {features_stats.loc['std'].max():.3f}]")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å
    correlation_matrix = df[features].corr()
    high_corr_pairs = []
    for i in range(len(features)):
        for j in range(i+1, len(features)):
            corr_val = abs(correlation_matrix.iloc[i, j])
            if corr_val > 0.8:
                high_corr_pairs.append((features[i], features[j], corr_val))
    
    if high_corr_pairs:
        print(f"\n‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—ã—Å–æ–∫–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (>0.8):")
        for f1, f2, corr in high_corr_pairs[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  {f1} - {f2}: {corr:.3f}")
    else:
        print("\n‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
    
    # 6. –ë–ê–õ–ê–ù–° –ö–û–í–ê–†–ò–ê–¢ –ú–ï–ñ–î–£ –ì–†–£–ü–ü–ê–ú–ò TREATMENT
    print("\n6. –ë–ê–õ–ê–ù–° –ö–û–í–ê–†–ò–ê–¢ –ú–ï–ñ–î–£ –ì–†–£–ü–ü–ê–ú–ò")
    print("-"*30)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è —Å—Ä–µ–¥–Ω–∏—Ö
    smd_results = []
    for feature in features:
        group_stats = df.groupby(treatment_var)[feature].agg(['mean', 'std'])
        if len(group_stats) == 2:
            mean_diff = abs(group_stats['mean'].iloc[0] - group_stats['mean'].iloc[1])
            pooled_std = np.sqrt((group_stats['std'].iloc[0]**2 + group_stats['std'].iloc[1]**2) / 2)
            smd = mean_diff / pooled_std if pooled_std > 0 else 0
            smd_results.append((feature, smd))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é SMD
    smd_results.sort(key=lambda x: x[1], reverse=True)
    
    print("–¢–æ–ø-5 —Ñ–∏—á–µ–π —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º (SMD):")
    for feature, smd in smd_results[:5]:
        status = "‚ö†Ô∏è" if smd > 0.25 else "‚úÖ"
        print(f"  {status} {feature}: SMD = {smd:.3f}")
    
    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –±–∞–ª–∞–Ω—Å–∞
    high_smd_count = sum(1 for _, smd in smd_results if smd > 0.25)
    print(f"\n–§–∏—á–µ–π —Å SMD > 0.25: {high_smd_count}/{len(features)}")
    
    # 7. –ê–ù–ê–õ–ò–ó –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô –î–õ–Ø NWOE/NIV
    print("\n7. –ü–†–û–í–ï–†–ö–ê –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô –î–õ–Ø NWOE/NIV")
    print("-"*30)
    
    # Instrumental Variable strength (–¥–ª—è NIV)
    print("–ê–Ω–∞–ª–∏–∑ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
    for var in network_vars:
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å treatment (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–Ω–∞—á–∏–º–æ–π)
        corr_treatment = df[var].corr(df[treatment_var])
        
        # –ß–∞—Å—Ç–∏—á–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å outcome (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ª–∞–±–æ–π –ø–æ—Å–ª–µ –∫–æ–Ω—Ç—Ä–æ–ª—è treatment)
        
        # –ü—Ä–æ—Å—Ç–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å outcome
        corr_outcome_simple = df[var].corr(df[outcome_var])
        
        print(f"  {var}:")
        print(f"    –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å treatment: {corr_treatment:.3f}")
        print(f"    –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å outcome: {corr_outcome_simple:.3f}")
        
        # –û—Ü–µ–Ω–∫–∞ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        if abs(corr_treatment) > 0.1:
            print(f"    ‚úÖ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–∏–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç")
        else:
            print(f"    ‚ö†Ô∏è  –°–ª–∞–±—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç")

# ..1 + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
def basic_eda_analysis(df, remove_duplicates=True, verbose=True):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (EDA)
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    ----------
    df : pandas.DataFrame
        –î–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    remove_duplicates : bool, default=True
        –£–¥–∞–ª—è—Ç—å –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞–π–¥–µ–Ω—ã
    verbose : bool, default=True
        –í—ã–≤–æ–¥–∏—Ç—å –ª–∏ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    -----------
    dict : —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    pandas.DataFrame : –æ—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º (–µ—Å–ª–∏ remove_duplicates=True)
    """
    
    if not isinstance(df, pd.DataFrame):
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å pandas DataFrame")
    
    results = {}
    
    # 1. –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    shape = df.shape
    results['shape'] = {
        'rows': shape[0],
        'columns': shape[1],
        'total_cells': shape[0] * shape[1]
    }
    
    if verbose:
        print("=== –†–ê–ó–ú–ï–† –î–ê–ù–ù–´–• ===")
        print(f"–°—Ç—Ä–æ–∫–∏: {shape[0]:,}")
        print(f"–°—Ç–æ–ª–±—Ü—ã: {shape[1]:,}")
        # print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è—á–µ–µ–∫: {shape[0] * shape[1]:,}")
        print()
    
    # 2. –ó–∞–Ω–∏–º–∞–µ–º–∞—è –ø–∞–º—è—Ç—å
    memory_usage = df.memory_usage(deep=True)
    total_memory_bytes = memory_usage.sum()
    total_memory_mb = total_memory_bytes / (1024 * 1024)
    
    results['memory'] = {
        'total_bytes': total_memory_bytes,
        'total_mb': round(total_memory_mb, 2),
        'by_column': memory_usage.to_dict()
    }
    
    if verbose:
        # print("=== –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ü–ê–ú–Ø–¢–ò ===")
        print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {total_memory_mb:.2f} MB ({total_memory_bytes:,} bytes)")
        # print("–ü–æ —Å—Ç–æ–ª–±—Ü–∞–º:")
        # for col, mem in memory_usage.items():
        #     if col != 'Index':
        #         print(f"  {col}: {mem/1024:.1f} KB")
        print()
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicates_count = df.duplicated().sum()
    duplicates_pct = (duplicates_count / len(df)) * 100 if len(df) > 0 else 0
    
    results['duplicates'] = {
        'count': duplicates_count,
        'percentage': round(duplicates_pct, 2),
        'removed': False
    }
    
    if verbose:
        print("=== –î–£–ë–õ–ò–ö–ê–¢–´ ===")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count:,}")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_pct:.2f}%")
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
    cleaned_df = df.copy()
    if remove_duplicates and duplicates_count > 0:
        cleaned_df = df.drop_duplicates()
        results['duplicates']['removed'] = True
        if verbose:
            print(f"–î—É–±–ª–∏–∫–∞—Ç—ã —É–¥–∞–ª–µ–Ω—ã. –ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {cleaned_df.shape[0]:,} —Å—Ç—Ä–æ–∫")
    
    if verbose:
        print()
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    missing_data = df.isnull().sum()
    missing_pct = (missing_data / len(df)) * 100 if len(df) > 0 else pd.Series()
    
    missing_summary = pd.DataFrame({
        'Missing_Count': missing_data,
        'Missing_Percentage': missing_pct
    }).round(2)
    
    total_missing = missing_data.sum()
    total_missing_pct = (total_missing / (df.shape[0] * df.shape[1])) * 100 if df.shape[0] * df.shape[1] > 0 else 0
    
    results['missing_values'] = {
        'total_missing': total_missing,
        'total_missing_percentage': round(total_missing_pct, 2),
        'by_column': missing_summary.to_dict('index'),
        'columns_with_missing': missing_data[missing_data > 0].index.tolist()
    }
    
    if verbose:
        print("=== –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø ===")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {total_missing:,}")
        print(f"–û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {total_missing_pct:.2f}%")
        
        if total_missing > 0:
            print("\n–ü–æ —Å—Ç–æ–ª–±—Ü–∞–º:")
            for col in missing_data[missing_data > 0].index:
                count = missing_data[col]
                pct = missing_pct[col]
                print(f"  {col}: {count:,} ({pct:.2f}%)")
        else:
            print("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        print()
    
    # 5. –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    dtypes_info = df.dtypes.value_counts()
    
    results['data_types'] = {
        'by_column': df.dtypes.to_dict(),
        'summary': dtypes_info.to_dict(),
        'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),
        'categorical_columns': df.select_dtypes(include=['object', 'category']).columns.tolist(),
        'datetime_columns': df.select_dtypes(include=['datetime64']).columns.tolist()
    }
    
    if verbose:
        print("=== –¢–ò–ü–´ –î–ê–ù–ù–´–• ===")
        print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö:")
        for dtype, count in dtypes_info.items():
            print(f"  {dtype}: {count} —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        print(f"\n–ß–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã ({len(results['data_types']['numeric_columns'])}): {results['data_types']['numeric_columns']}")
        print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã ({len(results['data_types']['categorical_columns'])}): {results['data_types']['categorical_columns']}")
        
        if results['data_types']['datetime_columns']:
            print(f"–°—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–∞–º–∏ ({len(results['data_types']['datetime_columns'])}): {results['data_types']['datetime_columns']}")
        print()

    return results, cleaned_df
#++++++–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–∞—Ö–∞ (IQR)
def detect_outliers(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = column[(column < lower_bound) | (column > upper_bound)]
    return outliers
class OutlierAnalyzer:#+
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self, df):
        self.df = df.copy()
        self.feature_cols = [col for col in df.columns if col.startswith('f')]
        self.outlier_info = {}
    
    def detect_outliers(self):
        """
        –í—ã—è–≤–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ –≤—Å–µ–º —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—ã–±—Ä–æ—Å–∞—Ö
        """
        outliers_summary = {}
        
        for col in self.feature_cols + ['conversion', 'visit', 'exposure']:
            if col in self.df.columns and np.issubdtype(self.df[col].dtype, np.number):
                outliers_summary[col] = self._detect_column_outliers(col)
        
        return outliers_summary
    
    def _detect_column_outliers(self, column):
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞"""
        data = self.df[column].dropna()
        
        # IQR –º–µ—Ç–æ–¥
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        iqr_outliers = data[(data < lower_bound) | (data > upper_bound)]
        
        return {
            'outliers_count': len(iqr_outliers),
            'outliers_percentage': len(iqr_outliers) / len(data) * 100,
            'bounds': (float(lower_bound), float(upper_bound)),
            'stats': {
                'mean': float(data.mean()),
                'median': float(data.median()),
                'std': float(data.std()),
                'min': float(data.min()),
                'max': float(data.max())
            }
        }
    # ..
    def generate_report(self):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –≤—ã–±—Ä–æ—Å–∞–º
        """
        outliers_summary = self.detect_outliers()
        total_outliers = sum(info['outliers_count'] for info in outliers_summary.values())
        
        report = "–ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:\n"
        
        for col, info in outliers_summary.items():
            report += (
                f"- {col} : {info['outliers_count']} –≤—ã–±—Ä–æ—Å–æ–≤ "
                f"({info['outliers_percentage']:.2f}%)\n"
            )
        
        report += f"\n–û–±—â–µ–µ –∫–æ–ª-–≤–æ –≤—ã–±—Ä–æ—Å–æ–≤: {total_outliers}\n"
        report += (
            f"* % –æ—Ç –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö: "
            f"{total_outliers / len(self.df) * 100:.2f}%\n\n"
        )        

        report += (
            "1. NWOE/NIV –º–µ—Ç–æ–¥—ã —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å '—Å—ã—Ä—ã–º–∏' –¥–∞–Ω–Ω—ã–º–∏ - –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–∏–Ω–Ω–∏–Ω–≥, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–Ω–∏–∂–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤\n"
            "2. Uplift-–º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º\n"
            "3. 76.53% –≤—ã–±—Ä–æ—Å–æ–≤ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞/–≤–µ–±-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏\n"
            "4. –í—ã–±—Ä–æ—Å—ã —Å–æ–¥–µ—Ä–∂–∞—Ç —Ü–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π "
        )
        
        return report

class NetworkInstrumentalVariables:#+
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Network Instrumental Variables (NIV)
    """
    
    def __init__(self, data: pd.DataFrame, treatment_col: str = 'treatment', 
                 outcome_col: str = 'conversion', network_vars: List[str] = ['visit', 'exposure']):
        self.data = data.copy()
        self.treatment_col = treatment_col
        self.outcome_col = outcome_col
        self.network_vars = network_vars
        self.feature_cols = [col for col in data.columns 
                           if col.startswith('f') and col not in [treatment_col, outcome_col] + network_vars]
        
    def check_instrument_validity(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        results = {}
        
        for iv in self.network_vars:
            # 1. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è IV —Å treatment
            relevance = np.corrcoef(self.data[iv], self.data[self.treatment_col])[0, 1]
            
            # 2. –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å: —á–∞—Å—Ç–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è IV —Å outcome –ø—Ä–∏ –∫–æ–Ω—Ç—Ä–æ–ª–µ treatment
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å—Ç–∞—Ç–∫–∏ –æ—Ç —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ outcome –Ω–∞ treatment
            model_outcome_treatment = LinearRegression()
            X_treatment = self.data[[self.treatment_col]]
            outcome_residuals = self.data[self.outcome_col] - model_outcome_treatment.fit(
                X_treatment, self.data[self.outcome_col]).predict(X_treatment)
            
            exclusivity = np.corrcoef(self.data[iv], outcome_residuals)[0, 1]
            
            # 3. F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            X_iv = self.data[[iv]]
            y_treatment = self.data[self.treatment_col]
            model_iv_treatment = LinearRegression().fit(X_iv, y_treatment)
            r2 = r2_score(y_treatment, model_iv_treatment.predict(X_iv))
            n = len(self.data)
            f_stat = (r2 / (1 - r2)) * (n - 2)
            
            results[iv] = {
                'relevance': relevance,
                'exclusivity': exclusivity,
                'f_statistic': f_stat,
                'weak_instrument': f_stat < 10  # –ü—Ä–∞–≤–∏–ª–æ Staiger-Stock
            }
            
        return results
