import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import pearsonr
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score
from sklearn.decomposition import PCA
from statsmodels.stats.weightstats import ttest_ind
from pylift.eval import UpliftEval
import loymax as lm
from typing import Dict, Tuple, List
import warnings
warnings.filterwarnings('ignore')

class NetworkWeightedOutcomeEstimation:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Network-based Weighted Outcome Estimation (NWOE)
    """
    
    def __init__(self, data: pd.DataFrame, treatment_col: str = 'treatment', 
                 outcome_col: str = 'conversion', network_vars: List[str] = ['visit', 'exposure']):
        self.data = data.copy()
        self.treatment_col = treatment_col
        self.outcome_col = outcome_col
        self.network_vars = network_vars
        self.feature_cols = [col for col in data.columns 
                           if col.startswith('f') and col not in [treatment_col, outcome_col] + network_vars]
        
    def calculate_network_weights(self, method: str = 'exposure_based') -> np.ndarray:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ—Ç–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        
        if method == 'exposure_based':
            # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ exposure: –±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è —É–∑–ª–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º exposure
            weights = 1 + self.data['exposure'].values
            
        elif method == 'visit_based':
            # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ visit: –±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è —É–∑–ª–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º visit
            weights = 1 + self.data['visit'].values
            
        elif method == 'combined':
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞
            exposure_norm = self.data['exposure'] / self.data['exposure'].max()
            visit_norm = self.data['visit'] / self.data['visit'].max()
            weights = 1 + 0.5 * (exposure_norm + visit_norm)
            
        elif method == 'inverse_variance':
            # –û–±—Ä–∞—Ç–Ω—ã–µ –≤–µ—Å–∞ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ (–¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤)
            network_score = self.data['exposure'] + self.data['visit']
            weights = 1 / (1 + np.var(network_score) * network_score)
            
        else:
            weights = np.ones(len(self.data))
            
        return weights / weights.sum()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    
    def propensity_score_weighting(self, use_network: bool = True) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä–æ–≤ —Å —É—á–µ—Ç–æ–º —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-–º–æ–¥–µ–ª–∏
        if use_network:
            X_prop = np.hstack([
                self.data[self.feature_cols].values,
                self.data[self.network_vars].values
            ])
        else:
            X_prop = self.data[self.feature_cols].values
            
        # –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-–º–æ–¥–µ–ª–∏
        prop_model = LogisticRegression(random_state=42, max_iter=1000)
        prop_model.fit(X_prop, self.data[self.treatment_col])
        propensity_scores = prop_model.predict_proba(X_prop)[:, 1]
        
        # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤
        propensity_scores = np.clip(propensity_scores, 0.01, 0.99)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IPW –≤–µ—Å–æ–≤
        treatment = self.data[self.treatment_col].values
        ipw_weights = treatment / propensity_scores + (1 - treatment) / (1 - propensity_scores)
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Ç–µ–≤—ã–µ –≤–µ—Å–∞, –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º –∏—Ö —Å IPW
        if use_network:
            network_weights = self.calculate_network_weights('combined')
            final_weights = ipw_weights * network_weights * len(self.data)
        else:
            final_weights = ipw_weights
            
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        final_weights = final_weights / final_weights.sum() * len(self.data)
        
        # –û—Ü–µ–Ω–∫–∞ ATE —Å –≤–µ—Å–∞–º–∏
        outcome = self.data[self.outcome_col].values
        treated_outcomes = outcome[treatment == 1]
        control_outcomes = outcome[treatment == 0]
        treated_weights = final_weights[treatment == 1]
        control_weights = final_weights[treatment == 0]
        
        ate_estimate = (np.average(treated_outcomes, weights=treated_weights) - 
                       np.average(control_outcomes, weights=control_weights))
        
        return {
            'ate_estimate': ate_estimate,
            'propensity_scores': propensity_scores,
            'weights': final_weights,
            'model': prop_model
        }
    
    def doubly_robust_estimation(self, sample_size: int = None, use_simple_models: bool = True) -> Dict:
        """–ë—ã—Å—Ç—Ä–∞—è –¥–≤–∞–∂–¥—ã —Ä–æ–±–∞—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å —Å–µ—Ç–µ–≤—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏"""
        
        print("–ó–∞–ø—É—Å–∫ Doubly Robust –æ—Ü–µ–Ω–∫–∏...")
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if sample_size and len(self.data) > sample_size:
            print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–º {sample_size:,} –∏–∑ {len(self.data):,}")
            sample_data = self.data.sample(n=sample_size, random_state=42)
        else:
            sample_data = self.data
            
        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_features = sample_data[self.feature_cols].values
        X_network = sample_data[self.network_vars].values
        X_prop = np.hstack([X_features, X_network])
        
        treatment = sample_data[self.treatment_col].values
        outcome = sample_data[self.outcome_col].values
        
        print("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-–º–æ–¥–µ–ª–∏...")
        # 1. –ü—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä—ã
        prop_model = LogisticRegression(random_state=42, max_iter=500, solver='lbfgs')
        prop_model.fit(X_prop, treatment)
        propensity_scores = prop_model.predict_proba(X_prop)[:, 1]
        propensity_scores = np.clip(propensity_scores, 0.01, 0.99)
        
        print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏—Å—Ö–æ–¥–æ–≤...")
        # 2. –ú–æ–¥–µ–ª–∏ –∏—Å—Ö–æ–¥–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        control_mask = treatment == 0
        treated_mask = treatment == 1
        
        if use_simple_models:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LinearRegression –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            control_model = LinearRegression()
            treated_model = LinearRegression()
        else:
            # RandomForest —Å –º–µ–Ω—å—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            control_model = RandomForestRegressor(n_estimators=50, max_depth=10, 
                                                n_jobs=-1, random_state=42)
            treated_model = RandomForestRegressor(n_estimators=50, max_depth=10, 
                                                n_jobs=-1, random_state=42)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        control_model.fit(X_prop[control_mask], outcome[control_mask])
        treated_model.fit(X_prop[treated_mask], outcome[treated_mask])
        
        print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        # 3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        mu0_hat = control_model.predict(X_prop)
        mu1_hat = treated_model.predict(X_prop)
        
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ ATE...")
        # 4. –î–≤–∞–∂–¥—ã —Ä–æ–±–∞—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        ipw_term1 = treatment * (outcome - mu1_hat) / propensity_scores
        ipw_term2 = (1 - treatment) * (outcome - mu0_hat) / (1 - propensity_scores)
        
        ate_estimate = np.mean(mu1_hat - mu0_hat) + np.mean(ipw_term1) - np.mean(ipw_term2)
        
        print("–û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
        return {
            'ate_estimate': ate_estimate,
            'mu0_predictions': mu0_hat,
            'mu1_predictions': mu1_hat,
            'propensity_scores': propensity_scores,
            'control_model': control_model,
            'treated_model': treated_model,
            'sample_size_used': len(sample_data)
        }
    
    def fast_nwoe_estimation(self, sample_size: int = 100000) -> Dict:
        """–ë—ã—Å—Ç—Ä–∞—è NWOE –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        
        print(f"‚ö° –ë—ã—Å—Ç—Ä–∞—è NWOE –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—ã–±–æ—Ä–∫–µ {sample_size:,}...")
        
        # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
        sample_data = self.data.groupby('treatment').apply(
            lambda x: x.sample(n=min(len(x), sample_size//2), random_state=42)
        ).reset_index(drop=True)
        
        print(f"üìä –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {len(sample_data):,}")
        print(f"   Treatment 0: {(sample_data['treatment']==0).sum():,}")
        print(f"   Treatment 1: {(sample_data['treatment']==1).sum():,}")
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-–º–æ–¥–µ–ª—å
        X_simple = sample_data[self.feature_cols[:6] + self.network_vars].values  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 6 —Ñ–∏—á–µ–π
        
        prop_model = LogisticRegression(random_state=42, max_iter=300)
        prop_model.fit(X_simple, sample_data[self.treatment_col])
        propensity_scores = prop_model.predict_proba(X_simple)[:, 1]
        propensity_scores = np.clip(propensity_scores, 0.05, 0.95)
        
        # –°–µ—Ç–µ–≤—ã–µ –≤–µ—Å–∞
        network_weights = 1 + 0.5 * (sample_data['exposure'] + sample_data['visit'])
        network_weights = network_weights / network_weights.sum() * len(sample_data)
        
        # IPW –≤–µ—Å–∞
        treatment = sample_data[self.treatment_col].values
        ipw_weights = treatment / propensity_scores + (1 - treatment) / (1 - propensity_scores)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞
        final_weights = ipw_weights * network_weights
        final_weights = np.clip(final_weights, 0.1, 10)  # –û–±—Ä–µ–∑–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤
        
        # ATE –æ—Ü–µ–Ω–∫–∞
        outcome = sample_data[self.outcome_col].values
        treated_outcomes = outcome[treatment == 1]
        control_outcomes = outcome[treatment == 0]
        treated_weights = final_weights[treatment == 1]
        control_weights = final_weights[treatment == 0]
        
        ate_estimate = (np.average(treated_outcomes, weights=treated_weights) - 
                       np.average(control_outcomes, weights=control_weights))
        
        print(f"‚úÖ –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: ATE = {ate_estimate:.6f}")
        
        return {
            'ate_estimate': ate_estimate,
            'propensity_scores': propensity_scores,
            'weights': final_weights,
            'sample_size': len(sample_data),
            'method': 'fast_nwoe'
        }

def comprehensive_ate_analysis(data: pd.DataFrame) -> Dict:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ATE —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
    
    print("üîç –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê ATE –° –°–ï–¢–ï–í–´–ú–ò –≠–§–§–ï–ö–¢–ê–ú–ò")
    print("=" * 60)
    
    results = {}
    
    # 1. –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–Ω–∏—Ö (–Ω–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
    naive_ate = (data[data['treatment'] == 1]['conversion'].mean() - 
                data[data['treatment'] == 0]['conversion'].mean())
    results['naive_ate'] = naive_ate
    
    print(f"üìä –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ATE: {naive_ate:.6f}")
    print()
    
    # 2. –ê–Ω–∞–ª–∏–∑ NIV
    print("üîß –ê–ù–ê–õ–ò–ó NETWORK INSTRUMENTAL VARIABLES (NIV)")
    print("-" * 40)
    
    niv_analyzer = NetworkInstrumentalVariables(data)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    iv_validity = niv_analyzer.check_instrument_validity()
    
    for iv, validity in iv_validity.items():
        print(f"üìà –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç '{iv}':")
        print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (–∫–æ—Ä—Ä. —Å treatment): {validity['relevance']:.4f}")
        print(f"   –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å (–æ—Å—Ç–∞—Ç. –∫–æ—Ä—Ä. —Å outcome): {validity['exclusivity']:.4f}")
        print(f"   F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {validity['f_statistic']:.2f}")
        print(f"   –°–ª–∞–±—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {'‚ö†Ô∏è  –î–∞' if validity['weak_instrument'] else '‚úÖ –ù–µ—Ç'}")
        print()
    
    # 2SLS –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    niv_estimates = {}
    for iv in ['visit', 'exposure']:
        tsls_result = niv_analyzer.two_stage_least_squares(iv)
        niv_estimates[iv] = tsls_result
        print(f"üéØ 2SLS –æ—Ü–µ–Ω–∫–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º '{iv}': {tsls_result['ate_estimate']:.6f}")
        print(f"   R¬≤ –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞: {tsls_result['first_stage_r2']:.4f}")
        print()
    
    results['niv_estimates'] = niv_estimates
    results['iv_validity'] = iv_validity
    
    # 3. –ê–Ω–∞–ª–∏–∑ NWOE
    print("üï∏Ô∏è  –ê–ù–ê–õ–ò–ó NETWORK-BASED WEIGHTED OUTCOME ESTIMATION (NWOE)")
    print("-" * 50)
    
    nwoe_analyzer = NetworkWeightedOutcomeEstimation(data)
    
    # –ü—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –±–µ–∑ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    psw_standard = nwoe_analyzer.propensity_score_weighting(use_network=False)
    print(f"‚öñÔ∏è  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ PSW: {psw_standard['ate_estimate']:.6f}")
    
    # –ü—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å —Å–µ—Ç–µ–≤—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏
    psw_network = nwoe_analyzer.propensity_score_weighting(use_network=True)
    print(f"üï∏Ô∏è  Network-enhanced PSW: {psw_network['ate_estimate']:.6f}")
    
    # –î–≤–∞–∂–¥—ã —Ä–æ–±–∞—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    dr_result = nwoe_analyzer.doubly_robust_estimation()
    print(f"üõ°Ô∏è  Doubly Robust: {dr_result['ate_estimate']:.6f}")
    print()
    
    results['nwoe_estimates'] = {
        'psw_standard': psw_standard,
        'psw_network': psw_network,
        'doubly_robust': dr_result
    }
    
    # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
    print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ú–ï–¢–û–î–û–í")
    print("-" * 30)
    
    all_estimates = {
        '–ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞': naive_ate,
        '2SLS (visit)': niv_estimates['visit']['ate_estimate'],
        '2SLS (exposure)': niv_estimates['exposure']['ate_estimate'],
        '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ PSW': psw_standard['ate_estimate'],
        'Network PSW': psw_network['ate_estimate'],
        'Doubly Robust': dr_result['ate_estimate']
    }
    
    for method, estimate in all_estimates.items():
        print(f"{method:15}: {estimate:8.6f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–∑–±—Ä–æ—Å–∞
    estimates_values = list(all_estimates.values())
    print()
    print(f"üìà –°—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –º–µ—Ç–æ–¥–∞–º: {np.mean(estimates_values):.6f}")
    print(f"üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(estimates_values):.6f}")
    print(f"üìè –†–∞–∑–º–∞—Ö: {np.max(estimates_values) - np.min(estimates_values):.6f}")
    
    results['all_estimates'] = all_estimates
    results['summary_stats'] = {
        'mean': np.mean(estimates_values),
        'std': np.std(estimates_values),
        'range': np.max(estimates_values) - np.min(estimates_values)
    }
    
    return results

def network_effects_diagnostics(data: pd.DataFrame) -> Dict:
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
    
    print("\nüî¨ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ï–¢–ï–í–´–• –≠–§–§–ï–ö–¢–û–í")
    print("=" * 40)
    
    diagnostics = {}
    
    # 1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    network_vars = ['visit', 'exposure']
    corr_matrix = data[['treatment', 'conversion'] + network_vars].corr()
    
    print("üìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞:")
    print(corr_matrix.round(4))
    print()
    
    # 2. –ê–Ω–∞–ª–∏–∑ spillover —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    print("üåä –ê–ù–ê–õ–ò–ó SPILLOVER –≠–§–§–ï–ö–¢–û–í")
    print("-" * 30)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º network exposure
    data['exposure_quartile'] = pd.qcut(data['exposure'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
    data['visit_quartile'] = pd.qcut(data['visit'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º exposure
    exposure_analysis = data.groupby(['exposure_quartile', 'treatment']).agg({
        'conversion': ['count', 'mean', 'std']
    }).round(6)
    
    print("üìà –ö–æ–Ω–≤–µ—Ä—Å–∏—è –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º exposure:")
    print(exposure_analysis)
    print()
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º visit
    visit_analysis = data.groupby(['visit_quartile', 'treatment']).agg({
        'conversion': ['count', 'mean', 'std']
    }).round(6)
    
    print("üìà –ö–æ–Ω–≤–µ—Ä—Å–∏—è –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º visit:")
    print(visit_analysis)
    print()
    
    # 3. –¢–µ—Å—Ç –Ω–∞ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    print("üîç –¢–ï–°–¢ –ù–ê –ì–ï–¢–ï–†–û–ì–ï–ù–ù–û–°–¢–¨ –≠–§–§–ï–ö–¢–û–í")
    print("-" * 35)
    
    # ATE –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º exposure
    ate_by_exposure = {}
    for q in ['Q1', 'Q2', 'Q3', 'Q4']:
        subset = data[data['exposure_quartile'] == q]
        ate_q = (subset[subset['treatment'] == 1]['conversion'].mean() - 
                subset[subset['treatment'] == 0]['conversion'].mean())
        ate_by_exposure[q] = ate_q
        print(f"ATE –≤ {q} –∫–≤–∞—Ä—Ç–∏–ª–µ exposure: {ate_q:.6f}")
    
    print()
    
    # ATE –ø–æ –∫–≤–∞—Ä—Ç–∏–ª—è–º visit
    ate_by_visit = {}
    for q in ['Q1', 'Q2', 'Q3', 'Q4']:
        subset = data[data['visit_quartile'] == q]
        ate_q = (subset[subset['treatment'] == 1]['conversion'].mean() - 
                subset[subset['treatment'] == 0]['conversion'].mean())
        ate_by_visit[q] = ate_q
        print(f"ATE –≤ {q} –∫–≤–∞—Ä—Ç–∏–ª–µ visit: {ate_q:.6f}")
    
    diagnostics['correlation_matrix'] = corr_matrix
    diagnostics['ate_by_exposure'] = ate_by_exposure  
    diagnostics['ate_by_visit'] = ate_by_visit
    diagnostics['exposure_analysis'] = exposure_analysis
    diagnostics['visit_analysis'] = visit_analysis
    
    return diagnostics

def create_visualization_plots(data: pd.DataFrame, results: Dict):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('–ê–Ω–∞–ª–∏–∑ ATE —Å —Å–µ—Ç–µ–≤—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏', fontsize=16, fontweight='bold')
    
    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ ATE
    methods = list(results['all_estimates'].keys())
    estimates = list(results['all_estimates'].values())
    
    axes[0, 0].barh(methods, estimates, color='skyblue', alpha=0.7)
    axes[0, 0].set_xlabel('ATE Estimate')
    axes[0, 0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ ATE')
    axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)
    
    # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    axes[0, 1].hist(data['visit'], bins=50, alpha=0.7, label='Visit', density=True)
    axes[0, 1].hist(data['exposure'], bins=50, alpha=0.7, label='Exposure', density=True)
    axes[0, 1].set_xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
    axes[0, 1].set_ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å')
    axes[0, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö')
    axes[0, 1].legend()
    
    # 3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
    network_corr = data[['treatment', 'conversion', 'visit', 'exposure']].corr()
    sns.heatmap(network_corr, annot=True, cmap='coolwarm', center=0, 
                ax=axes[0, 2], square=True)
    axes[0, 2].set_title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞')
    
    # 4. –ü—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä—ã
    psw_result = results['nwoe_estimates']['psw_network']
    axes[1, 0].hist(psw_result['propensity_scores'], bins=50, alpha=0.7, 
                   color='lightgreen', edgecolor='black')
    axes[1, 0].set_xlabel('Propensity Score')
    axes[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–ø–µ–Ω—Å–∏—Ç–∏-—Å–∫–æ—Ä–æ–≤')
    
    # 5. Conversion rate –ø–æ –≥—Ä—É–ø–ø–∞–º –∏ —Å–µ—Ç–µ–≤—ã–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
    data['exposure_bin'] = pd.cut(data['exposure'], bins=10, labels=False)
    grouped_data = data.groupby(['exposure_bin', 'treatment'])['conversion'].mean().reset_index()
    
    for treatment in [0, 1]:
        subset = grouped_data[grouped_data['treatment'] == treatment]
        axes[1, 1].plot(subset['exposure_bin'], subset['conversion'], 
                       marker='o', label=f'Treatment {treatment}')
    
    axes[1, 1].set_xlabel('Exposure Bin')
    axes[1, 1].set_ylabel('Conversion Rate')
    axes[1, 1].set_title('Conversion Rate –ø–æ —É—Ä–æ–≤–Ω—è–º Exposure')
    axes[1, 1].legend()
    
    # 6. –û—Å—Ç–∞—Ç–∫–∏ –º–æ–¥–µ–ª–∏
    dr_result = results['nwoe_estimates']['doubly_robust']
    treatment_mask = data['treatment'] == 1
    
    residuals_treated = (data.loc[treatment_mask, 'conversion'].values - 
                        dr_result['mu1_predictions'][treatment_mask])
    residuals_control = (data.loc[~treatment_mask, 'conversion'].values - 
                        dr_result['mu0_predictions'][~treatment_mask])
    
    axes[1, 2].scatter(dr_result['mu1_predictions'][treatment_mask][:1000], 
                      residuals_treated[:1000], alpha=0.5, label='Treated', s=1)
    axes[1, 2].scatter(dr_result['mu0_predictions'][~treatment_mask][:1000], 
                      residuals_control[:1000], alpha=0.5, label='Control', s=1)
    axes[1, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
    axes[1, 2].set_xlabel('Predicted Values')
    axes[1, 2].set_ylabel('Residuals')
    axes[1, 2].set_title('–û—Å—Ç–∞—Ç–∫–∏ –º–æ–¥–µ–ª–∏ (sample)')
    axes[1, 2].legend()
    
    plt.tight_layout()
    plt.show()
    
    return fig

# –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
def quick_nwoe_analysis(data: pd.DataFrame, sample_size: int = 100000) -> Dict:
    """–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ NWOE –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    print("‚ö° –ë–´–°–¢–†–´–ô NWOE –ê–ù–ê–õ–ò–ó")
    print("=" * 30)
    
    # –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    naive_ate = (data[data['treatment'] == 1]['conversion'].mean() - 
                data[data['treatment'] == 0]['conversion'].mean())
    print(f"üìä –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ATE: {naive_ate:.6f}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NWOE –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    nwoe = NetworkWeightedOutcomeEstimation(data)
    
    # –ë—ã—Å—Ç—Ä—ã–µ –º–µ—Ç–æ–¥—ã
    print("\nüöÄ –ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä—ã—Ö –º–µ—Ç–æ–¥–æ–≤...")
    
    # 1. –ë—ã—Å—Ç—Ä–∞—è NWOE –æ—Ü–µ–Ω–∫–∞
    fast_result = nwoe.fast_nwoe_estimation(sample_size)
    
    # 2. –ë—ã—Å—Ç—Ä–∞—è Doubly Robust —Å –ª–∏–Ω–µ–π–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
    dr_fast = nwoe.doubly_robust_estimation(sample_size, use_simple_models=True)
    
    # 3. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ PSW –Ω–∞ –≤—ã–±–æ—Ä–∫–µ
    sample_data = data.sample(n=min(len(data), sample_size), random_state=42)
    nwoe_sample = NetworkWeightedOutcomeEstimation(sample_data)
    psw_result = nwoe_sample.propensity_score_weighting(use_network=True)
    
    results = {
        'naive_ate': naive_ate,
        'fast_nwoe': fast_result['ate_estimate'],
        'doubly_robust_fast': dr_fast['ate_estimate'],
        'psw_network': psw_result['ate_estimate']
    }
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   –ù–∞–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:     {naive_ate:.6f}")
    print(f"   –ë—ã—Å—Ç—Ä–∞—è NWOE:       {fast_result['ate_estimate']:.6f}")
    print(f"   Doubly Robust:      {dr_fast['ate_estimate']:.6f}")
    print(f"   Network PSW:        {psw_result['ate_estimate']:.6f}")
    
    # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    estimates = list(results.values())
    std_dev = np.std(estimates)
    print(f"\nüìà –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_dev:.6f}")
    
    if std_dev < 0.001:
        print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã")
    else:
        print("‚ö†Ô∏è  –ï—Å—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏")
    
    return {
        'estimates': results,
        'details': {
            'fast_nwoe': fast_result,
            'doubly_robust': dr_fast,
            'psw_network': psw_result
        },
        'consistency': std_dev
    }
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    print("üöÄ –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó NWOE –ò NIV –î–õ–Ø –û–¶–ï–ù–ö–ò ATE")
    print("=" * 50)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
    results = comprehensive_ate_analysis(data)
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    diagnostics = network_effects_diagnostics(data)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    create_visualization_plots(data, results)
    
    # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ò –í–´–í–û–î–´")
    print("=" * 30)
    
    estimates = results['all_estimates']
    std_estimates = results['summary_stats']['std']
    
    if std_estimates < 0.001:
        print("‚úÖ –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏ - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã")
    else:
        print("‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏ - —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    weak_instruments = [iv for iv, validity in results['iv_validity'].items() 
                       if validity['weak_instrument']]
    
    if weak_instruments:
        print(f"üîß –°–ª–∞–±—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã: {weak_instruments}")
        print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ NWOE –º–µ—Ç–æ–¥—ã")
    else:
        print("‚úÖ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–ª—å–Ω—ã –¥–ª—è NIV –∞–Ω–∞–ª–∏–∑–∞")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –ª—É—á—à–µ–º—É –º–µ—Ç–æ–¥—É
    dr_estimate = estimates['Doubly Robust']
    network_psw_estimate = estimates['Network PSW']
    
    print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–ê–Ø –û–¶–ï–ù–ö–ê ATE:")
    print(f"   Doubly Robust: {dr_estimate:.6f}")
    print(f"   (—Å —É—á–µ—Ç–æ–º —Å–µ—Ç–µ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤)")
    
    return {
        'results': results,
        'diagnostics': diagnostics,
        'recommendation': {
            'best_estimate': dr_estimate,
            'method': 'Doubly Robust',
            'confidence': 'high' if std_estimates < 0.001 else 'medium'
        }
    }



# eda
#+
def comprehensive_eda_for_ate_analysis(df):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π EDA –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ NWOE –∏ NIV –∞–Ω–∞–ª–∏–∑—É ATE
    """
    
    print("="*60)
    print("–ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô EDA –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê ATE (NWOE/NIV)")
    print("="*60)
    
    # 1. –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–ù–ù–´–•
    print("\n1. –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø")
    print("-"*30)
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {len(df)}")
    
    # –í—ã–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    features = [col for col in df.columns if col.startswith('f')]
    treatment_var = 'treatment'
    outcome_var = 'conversion'
    network_vars = ['visit', 'exposure']
    
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π: {len(features)}")
    print(f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è treatment: {treatment_var}")
    print(f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è outcome: {outcome_var}")
    print(f"–°–µ—Ç–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {network_vars}")
    
    # 2. –ê–ù–ê–õ–ò–ó –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø TREATMENT
    print("\n2. –ê–ù–ê–õ–ò–ó TREATMENT –ì–†–£–ü–ü–´")
    print("-"*30)
    treatment_dist = df[treatment_var].value_counts().sort_index()
    treatment_prop = df[treatment_var].value_counts(normalize=True).sort_index()
    
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ treatment:")
    for val, count, prop in zip(treatment_dist.index, treatment_dist.values, treatment_prop.values):
        print(f"  Treatment {val}: {count} ({prop:.3f})")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å
    min_group_size = treatment_dist.min()
    max_group_size = treatment_dist.max()
    imbalance_ratio = max_group_size / min_group_size
    print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {imbalance_ratio:.2f}")
    if imbalance_ratio > 3:
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –≤ –≥—Ä—É–ø–ø–∞—Ö treatment!")
    
    # 3. –ê–ù–ê–õ–ò–ó OUTCOME –ü–ï–†–ï–ú–ï–ù–ù–û–ô
    print("\n3. –ê–ù–ê–õ–ò–ó OUTCOME (CONVERSION)")
    print("-"*30)
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ outcome
    outcome_stats = df[outcome_var].describe()
    print("–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ conversion:")
    print(outcome_stats)
    
    # Outcome –ø–æ –≥—Ä—É–ø–ø–∞–º treatment
    print("\nConversion –ø–æ –≥—Ä—É–ø–ø–∞–º treatment:")
    outcome_by_treatment = df.groupby(treatment_var)[outcome_var].agg(['count', 'mean', 'std'])
    print(outcome_by_treatment)
    
    # –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ outcome –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏
    treatment_groups = [group[outcome_var].values for name, group in df.groupby(treatment_var)]
    if len(treatment_groups) == 2:
        t_stat, p_val = stats.ttest_ind(treatment_groups[0], treatment_groups[1])
        print(f"\nT-test –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏: t={t_stat:.3f}, p={p_val:.4f}")
    
    # 4. –ê–ù–ê–õ–ò–ó –°–ï–¢–ï–í–´–• –ü–ï–†–ï–ú–ï–ù–ù–´–•
    print("\n4. –ê–ù–ê–õ–ò–ó –°–ï–¢–ï–í–´–• –ü–ï–†–ï–ú–ï–ù–ù–´–•")
    print("-"*30)
    
    for var in network_vars:
        print(f"\n{var.upper()}:")
        var_stats = df[var].describe()
        print(f"  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={var_stats['mean']:.3f}, std={var_stats['std']:.3f}")
        print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: [{var_stats['min']:.1f}, {var_stats['max']:.1f}]")
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å treatment –∏ outcome
        corr_treatment = df[var].corr(df[treatment_var])
        corr_outcome = df[var].corr(df[outcome_var])
        print(f"  –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å treatment: {corr_treatment:.3f}")
        print(f"  –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å outcome: {corr_outcome:.3f}")
    
    # 5. –ê–ù–ê–õ–ò–ó –ö–û–í–ê–†–ò–ê–¢ (FEATURES)
    print("\n5. –ê–ù–ê–õ–ò–ó –ö–û–í–ê–†–ò–ê–¢ (FEATURES)")
    print("-"*30)
    
    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏—á–µ–π
    features_stats = df[features].describe()
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ {len(features)} —Ñ–∏—á–∞–º:")
    print(f"  –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: [{features_stats.loc['mean'].min():.3f}, {features_stats.loc['mean'].max():.3f}]")
    print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è: [{features_stats.loc['std'].min():.3f}, {features_stats.loc['std'].max():.3f}]")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å
    correlation_matrix = df[features].corr()
    high_corr_pairs = []
    for i in range(len(features)):
        for j in range(i+1, len(features)):
            corr_val = abs(correlation_matrix.iloc[i, j])
            if corr_val > 0.8:
                high_corr_pairs.append((features[i], features[j], corr_val))
    
    if high_corr_pairs:
        print(f"\n‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—ã—Å–æ–∫–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (>0.8):")
        for f1, f2, corr in high_corr_pairs[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  {f1} - {f2}: {corr:.3f}")
    else:
        print("\n‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
    
    # 6. –ë–ê–õ–ê–ù–° –ö–û–í–ê–†–ò–ê–¢ –ú–ï–ñ–î–£ –ì–†–£–ü–ü–ê–ú–ò TREATMENT
    print("\n6. –ë–ê–õ–ê–ù–° –ö–û–í–ê–†–ò–ê–¢ –ú–ï–ñ–î–£ –ì–†–£–ü–ü–ê–ú–ò")
    print("-"*30)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è —Å—Ä–µ–¥–Ω–∏—Ö
    smd_results = []
    for feature in features:
        group_stats = df.groupby(treatment_var)[feature].agg(['mean', 'std'])
        if len(group_stats) == 2:
            mean_diff = abs(group_stats['mean'].iloc[0] - group_stats['mean'].iloc[1])
            pooled_std = np.sqrt((group_stats['std'].iloc[0]**2 + group_stats['std'].iloc[1]**2) / 2)
            smd = mean_diff / pooled_std if pooled_std > 0 else 0
            smd_results.append((feature, smd))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é SMD
    smd_results.sort(key=lambda x: x[1], reverse=True)
    
    print("–¢–æ–ø-5 —Ñ–∏—á–µ–π —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º (SMD):")
    for feature, smd in smd_results[:5]:
        status = "‚ö†Ô∏è" if smd > 0.25 else "‚úÖ"
        print(f"  {status} {feature}: SMD = {smd:.3f}")
    
    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –±–∞–ª–∞–Ω—Å–∞
    high_smd_count = sum(1 for _, smd in smd_results if smd > 0.25)
    print(f"\n–§–∏—á–µ–π —Å SMD > 0.25: {high_smd_count}/{len(features)}")
    
    # 7. –ê–ù–ê–õ–ò–ó –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô –î–õ–Ø NWOE/NIV
    print("\n7. –ü–†–û–í–ï–†–ö–ê –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô –î–õ–Ø NWOE/NIV")
    print("-"*30)
    
    # Instrumental Variable strength (–¥–ª—è NIV)
    print("–ê–Ω–∞–ª–∏–∑ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
    for var in network_vars:
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å treatment (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–Ω–∞—á–∏–º–æ–π)
        corr_treatment = df[var].corr(df[treatment_var])
        
        # –ß–∞—Å—Ç–∏—á–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å outcome (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ª–∞–±–æ–π –ø–æ—Å–ª–µ –∫–æ–Ω—Ç—Ä–æ–ª—è treatment)
        
        # –ü—Ä–æ—Å—Ç–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å outcome
        corr_outcome_simple = df[var].corr(df[outcome_var])
        
        print(f"  {var}:")
        print(f"    –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å treatment: {corr_treatment:.3f}")
        print(f"    –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å outcome: {corr_outcome_simple:.3f}")
        
        # –û—Ü–µ–Ω–∫–∞ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        if abs(corr_treatment) > 0.1:
            print(f"    ‚úÖ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–∏–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç")
        else:
            print(f"    ‚ö†Ô∏è  –°–ª–∞–±—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç")

#+# 1 + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
def basic_eda_analysis(df, remove_duplicates=True, verbose=True):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (EDA)
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    ----------
    df : pandas.DataFrame
        –î–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    remove_duplicates : bool, default=True
        –£–¥–∞–ª—è—Ç—å –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞–π–¥–µ–Ω—ã
    verbose : bool, default=True
        –í—ã–≤–æ–¥–∏—Ç—å –ª–∏ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    -----------
    dict : —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    pandas.DataFrame : –æ—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º (–µ—Å–ª–∏ remove_duplicates=True)
    """
    
    if not isinstance(df, pd.DataFrame):
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å pandas DataFrame")
    
    results = {}
    
    # 1. –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    shape = df.shape
    results['shape'] = {
        'rows': shape[0],
        'columns': shape[1],
        'total_cells': shape[0] * shape[1]
    }
    
    if verbose:
        print("=== –†–ê–ó–ú–ï–† –î–ê–ù–ù–´–• ===")
        print(f"–°—Ç—Ä–æ–∫–∏: {shape[0]:,}")
        print(f"–°—Ç–æ–ª–±—Ü—ã: {shape[1]:,}")
        # print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è—á–µ–µ–∫: {shape[0] * shape[1]:,}")
        print()
    
    # 2. –ó–∞–Ω–∏–º–∞–µ–º–∞—è –ø–∞–º—è—Ç—å
    memory_usage = df.memory_usage(deep=True)
    total_memory_bytes = memory_usage.sum()
    total_memory_mb = total_memory_bytes / (1024 * 1024)
    
    results['memory'] = {
        'total_bytes': total_memory_bytes,
        'total_mb': round(total_memory_mb, 2),
        'by_column': memory_usage.to_dict()
    }
    
    if verbose:
        # print("=== –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ü–ê–ú–Ø–¢–ò ===")
        print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {total_memory_mb:.2f} MB ({total_memory_bytes:,} bytes)")
        # print("–ü–æ —Å—Ç–æ–ª–±—Ü–∞–º:")
        # for col, mem in memory_usage.items():
        #     if col != 'Index':
        #         print(f"  {col}: {mem/1024:.1f} KB")
        print()
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicates_count = df.duplicated().sum()
    duplicates_pct = (duplicates_count / len(df)) * 100 if len(df) > 0 else 0
    
    results['duplicates'] = {
        'count': duplicates_count,
        'percentage': round(duplicates_pct, 2),
        'removed': False
    }
    
    if verbose:
        print("=== –î–£–ë–õ–ò–ö–ê–¢–´ ===")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count:,}")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_pct:.2f}%")
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
    cleaned_df = df.copy()
    if remove_duplicates and duplicates_count > 0:
        cleaned_df = df.drop_duplicates()
        results['duplicates']['removed'] = True
        if verbose:
            print(f"–î—É–±–ª–∏–∫–∞—Ç—ã —É–¥–∞–ª–µ–Ω—ã. –ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {cleaned_df.shape[0]:,} —Å—Ç—Ä–æ–∫")
    
    if verbose:
        print()
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    missing_data = df.isnull().sum()
    missing_pct = (missing_data / len(df)) * 100 if len(df) > 0 else pd.Series()
    
    missing_summary = pd.DataFrame({
        'Missing_Count': missing_data,
        'Missing_Percentage': missing_pct
    }).round(2)
    
    total_missing = missing_data.sum()
    total_missing_pct = (total_missing / (df.shape[0] * df.shape[1])) * 100 if df.shape[0] * df.shape[1] > 0 else 0
    
    results['missing_values'] = {
        'total_missing': total_missing,
        'total_missing_percentage': round(total_missing_pct, 2),
        'by_column': missing_summary.to_dict('index'),
        'columns_with_missing': missing_data[missing_data > 0].index.tolist()
    }
    
    if verbose:
        print("=== –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø ===")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {total_missing:,}")
        print(f"–û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {total_missing_pct:.2f}%")
        
        if total_missing > 0:
            print("\n–ü–æ —Å—Ç–æ–ª–±—Ü–∞–º:")
            for col in missing_data[missing_data > 0].index:
                count = missing_data[col]
                pct = missing_pct[col]
                print(f"  {col}: {count:,} ({pct:.2f}%)")
        else:
            print("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        print()
    
    # 5. –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    dtypes_info = df.dtypes.value_counts()
    
    results['data_types'] = {
        'by_column': df.dtypes.to_dict(),
        'summary': dtypes_info.to_dict(),
        'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),
        'categorical_columns': df.select_dtypes(include=['object', 'category']).columns.tolist(),
        'datetime_columns': df.select_dtypes(include=['datetime64']).columns.tolist()
    }
    
    if verbose:
        print("=== –¢–ò–ü–´ –î–ê–ù–ù–´–• ===")
        print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö:")
        for dtype, count in dtypes_info.items():
            print(f"  {dtype}: {count} —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        print(f"\n–ß–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã ({len(results['data_types']['numeric_columns'])}): {results['data_types']['numeric_columns']}")
        print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã ({len(results['data_types']['categorical_columns'])}): {results['data_types']['categorical_columns']}")
        
        if results['data_types']['datetime_columns']:
            print(f"–°—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–∞–º–∏ ({len(results['data_types']['datetime_columns'])}): {results['data_types']['datetime_columns']}")
        print()

    return results, cleaned_df
#++++++–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–∞—Ö–∞ (IQR)
def detect_outliers(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = column[(column < lower_bound) | (column > upper_bound)]
    return outliers
class OutlierAnalyzer:#+
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self, df):
        self.df = df.copy()
        self.feature_cols = [col for col in df.columns if col.startswith('f')]
        self.outlier_info = {}
    
    def detect_outliers(self):
        """
        –í—ã—è–≤–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ –≤—Å–µ–º —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—ã–±—Ä–æ—Å–∞—Ö
        """
        outliers_summary = {}
        
        for col in self.feature_cols + ['conversion', 'visit', 'exposure']:
            if col in self.df.columns and np.issubdtype(self.df[col].dtype, np.number):
                outliers_summary[col] = self._detect_column_outliers(col)
        
        return outliers_summary
    
    def _detect_column_outliers(self, column):
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞"""
        data = self.df[column].dropna()
        
        # IQR –º–µ—Ç–æ–¥
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        iqr_outliers = data[(data < lower_bound) | (data > upper_bound)]
        
        return {
            'outliers_count': len(iqr_outliers),
            'outliers_percentage': len(iqr_outliers) / len(data) * 100,
            'bounds': (float(lower_bound), float(upper_bound)),
            'stats': {
                'mean': float(data.mean()),
                'median': float(data.median()),
                'std': float(data.std()),
                'min': float(data.min()),
                'max': float(data.max())
            }
        }
    
    def generate_report(self):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –≤—ã–±—Ä–æ—Å–∞–º
        """
        outliers_summary = self.detect_outliers()
        total_outliers = sum(info['outliers_count'] for info in outliers_summary.values())
        
        report = "–ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:\n"
        
        for col, info in outliers_summary.items():
            report += (
                f"- {col} : {info['outliers_count']} –≤—ã–±—Ä–æ—Å–æ–≤ "
                f"({info['outliers_percentage']:.2f}%)\n"
            )
        
        report += f"\n–û–±—â–µ–µ –∫–æ–ª-–≤–æ –≤—ã–±—Ä–æ—Å–æ–≤: {total_outliers}\n"
        report += (
            f"* % –æ—Ç –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö: "
            f"{total_outliers / len(self.df) * 100:.2f}%\n\n"
        )        

        report += (
            "1. NWOE/NIV –º–µ—Ç–æ–¥—ã —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å '—Å—ã—Ä—ã–º–∏' –¥–∞–Ω–Ω—ã–º–∏ - –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–∏–Ω–Ω–∏–Ω–≥, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–Ω–∏–∂–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤\n"
            "2. Uplift-–º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º\n"
            "3. 76.53% –≤—ã–±—Ä–æ—Å–æ–≤ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞/–≤–µ–±-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏\n"
            "4. –í—ã–±—Ä–æ—Å—ã —Å–æ–¥–µ—Ä–∂–∞—Ç —Ü–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π "
        )
        
        return report

class NetworkInstrumentalVariables:#+
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Network Instrumental Variables (NIV)
    """
    
    def __init__(self, data: pd.DataFrame, treatment_col: str = 'treatment', 
                 outcome_col: str = 'conversion', network_vars: List[str] = ['visit', 'exposure']):
        self.data = data.copy()
        self.treatment_col = treatment_col
        self.outcome_col = outcome_col
        self.network_vars = network_vars
        self.feature_cols = [col for col in data.columns 
                           if col.startswith('f') and col not in [treatment_col, outcome_col] + network_vars]
        
    def check_instrument_validity(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        results = {}
        
        for iv in self.network_vars:
            # 1. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è IV —Å treatment
            relevance = np.corrcoef(self.data[iv], self.data[self.treatment_col])[0, 1]
            
            # 2. –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å: —á–∞—Å—Ç–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è IV —Å outcome –ø—Ä–∏ –∫–æ–Ω—Ç—Ä–æ–ª–µ treatment
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å—Ç–∞—Ç–∫–∏ –æ—Ç —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ outcome –Ω–∞ treatment
            model_outcome_treatment = LinearRegression()
            X_treatment = self.data[[self.treatment_col]]
            outcome_residuals = self.data[self.outcome_col] - model_outcome_treatment.fit(
                X_treatment, self.data[self.outcome_col]).predict(X_treatment)
            
            exclusivity = np.corrcoef(self.data[iv], outcome_residuals)[0, 1]
            
            # 3. F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            X_iv = self.data[[iv]]
            y_treatment = self.data[self.treatment_col]
            model_iv_treatment = LinearRegression().fit(X_iv, y_treatment)
            r2 = r2_score(y_treatment, model_iv_treatment.predict(X_iv))
            n = len(self.data)
            f_stat = (r2 / (1 - r2)) * (n - 2)
            
            results[iv] = {
                'relevance': relevance,
                'exclusivity': exclusivity,
                'f_statistic': f_stat,
                'weak_instrument': f_stat < 10  # –ü—Ä–∞–≤–∏–ª–æ Staiger-Stock
            }
            
        return results